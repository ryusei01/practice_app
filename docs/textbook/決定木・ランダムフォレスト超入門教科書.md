# 実務エンジニアのための

# 決定木・ランダムフォレスト超入門教科書

本書は **「決定木が何かわからないレベル」** から、
**実務でコードを書いて使えるレベル** まで到達することを目的とします。

---

## 1. 決定木とは何か（超直感）

### 1.1 一言で言うと

決定木（Decision Tree）とは、

> **質問を順番にしていき、条件分岐で結論を出すモデル**

です。

人間が無意識にやっている「判断」を、そのまま機械にさせます。

---

## 2. 日常例で理解する決定木

### 2.1 人間の判断はすでに決定木

たとえば「外出するか」を決める場合：

- 雨が降っている？

  - はい → 外出しない
  - いいえ → 次へ

- 気温が低い？

  - はい → 上着を着て外出
  - いいえ → そのまま外出

この **質問 → 分岐 → 結論** の構造が決定木です。

---

## 3. データになるとどう見えるか

### 3.1 シンプルな表（元データ）

まずは、決定木で扱う **最小限のデータ** を考えます。

| 気温 | 雨     | 外出     |
| ---- | ------ | -------- |
| 高い | いいえ | 行く     |
| 低い | はい   | 行かない |
| 高い | はい   | 行かない |

- **気温・雨**：入力（特徴量）
- **外出**：予測したい答え（目的変数）

ここでは、

> 「気温」と「雨」から **外出するかどうか** を予測する

という問題設定になります。

---

### 3.2 決定木はこの表をどう見るか（考え方）

決定木は次のように自問します。

- 最初にどんな質問をすると、答えが一番きれいに分かれるか？

候補となる質問は 2 つです。

1. 気温は高い？低い？
2. 雨は降っている？いない？

---

### 3.3 質問 ①「気温」で分けてみる

- 気温 = 高い

  - 行く
  - 行かない

→ **答えが混ざっている**

- 気温 = 低い

  - 行かない

→ こちらはきれいだが、全体としては微妙

---

### 3.4 質問 ②「雨」で分けてみる

- 雨 = はい

  - 行かない
  - 行かない

→ **完全に一致**

- 雨 = いいえ

  - 行く

→ **完全に一致**

👉 この時点で、

> 「雨かどうか」で分けるのが最善

と判断されます。

---

### 3.5 これが「不純度が小さい」という意味

- 分岐後のグループが

  - ほぼ同じ答え → 不純度が小さい
  - バラバラ → 不純度が大きい

決定木は **数式を使って** これを計算し、
一番小さくなる質問を自動で選びます。

---

### 3.6 実際のコードに落とす（超重要）

この表を **Python コード** にするとこうなります。

```python
import pandas as pd
from sklearn.tree import DecisionTreeClassifier

# データ作成
X = pd.DataFrame({
    "気温": [1, 0, 1],      # 高い=1, 低い=0
    "雨": [0, 1, 1]        # いいえ=0, はい=1
})

y = [1, 0, 0]  # 行く=1, 行かない=0

# モデル作成
model = DecisionTreeClassifier(max_depth=2)
model.fit(X, y)
```

ポイント：

- 文字列は **数値に変換** する
- これを特徴量エンコーディングと言う

---

### 3.7 予測してみる（演習）

**問題**：

- 気温が高い
- 雨が降っていない

このとき外出するか？

```python
new_data = pd.DataFrame({
    "気温": [1],
    "雨": [0]
})

prediction = model.predict(new_data)
print(prediction)
```

**考え方**：

- 雨 = いいえ
- → 行く

**出力イメージ**：

```
[1]
```

---

### 3.8 決定木の中身を言葉で再現すると

このモデルは、内部的にほぼ次のルールを持っています。

- 雨 = はい → 行かない
- 雨 = いいえ → 行く

👉 人間が書いた if 文と同じです。

---

### 3.9 if 文で書くとこうなる

```python
if 雨 == 1:
    外出 = 0
else:
    外出 = 1
```

決定木は、この if 文を
**データから自動生成** してくれる仕組みです。

---|---|---|
| 高い | いいえ | 行く |
| 低い | はい | 行かない |
| 高い | はい | 行かない |

決定木は次を自動で考えます。

- 最初にどの質問をすれば一番きれいに分かれるか？

この例では **「雨かどうか」** が最初の質問として優秀です。

---

## 4. 良い分け方・悪い分け方

### 4.1 ポイントは「混ざり具合」

- 1 つの答えに揃っている → 良い分け方
- いろいろ混ざっている → 悪い分け方

この「混ざり具合」を数値にしたものが **不純度** です。

---

## 5. Gini 不純度（感覚だけで OK）

- 0 に近い → ほぼ同じ答え
- 大きい → ごちゃごちゃ

決定木は：

> **分岐後の不純度が一番小さくなる質問**

を選び続けます。

※ 数式は実務では不要です。

---

## 6. 木が成長するとはどういうことか

決定木は以下を繰り返します。

1. 一番良い質問を選ぶ
2. データを分ける
3. それぞれでまた質問する

質問の回数が増えるほど **木が深くなる** と言います。

---

## 7. なぜ過学習しやすいのか

質問を増やしすぎると：

- 1 件のデータ専用ルール
- ノイズを丸暗記

が起こります。

これが **過学習** です。

---

## 8. 深さ制限（最重要）

```python
from sklearn.tree import DecisionTreeClassifier

model = DecisionTreeClassifier(max_depth=3)
```

意味：

- 質問は最大 3 回まで

効果：

- ルールが単純
- 新しいデータに強い
- 説明しやすい

---

## 9. 最小コードで動かす決定木

```python
from sklearn.tree import DecisionTreeClassifier

model = DecisionTreeClassifier(max_depth=3)
model.fit(X_train, y_train)
pred = model.predict(X_test)
```

これだけで「自動判断マシン」が完成します。

---

## 10. 決定木の長所と短所

### 長所

- 前処理が少ない
- 判断理由を説明できる
- 小規模データに強い

### 短所

- 過学習しやすい
- 単体では精度に限界

---

## 11. ランダムフォレストとは何か

### 11.1 一言で

> **少しずつ違う決定木を大量に作って、多数決を取るモデル**

---

## 12. なぜ強くなるのか

### 12.1 2 つのランダム

1. データをランダム抽出（ブートストラップ）
2. 使う特徴量をランダム制限

→ 同じ木ができにくい

### 12.2 効果

- ミスが相殺される
- 精度が安定
- 過学習しにくい

---

## 13. 最小コードで動かすランダムフォレスト

```python
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(
    n_estimators=100,
    max_depth=5,
    random_state=42
)

model.fit(X_train, y_train)
pred = model.predict(X_test)
```

---

## 14. 超重要パラメータ

- n_estimators：木の本数（多いほど安定、重い）
- max_depth：1 本の木の深さ（深すぎ注意）

---

## 15. 決定木 vs ランダムフォレスト

| 項目     | 決定木 | ランダムフォレスト |
| -------- | ------ | ------------------ |
| 解釈性   | 高い   | やや低い           |
| 精度     | 低め   | 高い               |
| 安定性   | 低い   | 高い               |
| 実務使用 | 補助   | 主力               |

---

## 16. 演習 ①：深さを変える

```python
DecisionTreeClassifier(max_depth=10)
```

起きやすいこと：

- 学習精度は上がる
- テスト精度は下がる
- 過学習

---

## 17. 演習 ②：どちらが良い？

A

```python
RandomForestClassifier(n_estimators=10, max_depth=20)
```

B

```python
RandomForestClassifier(n_estimators=100, max_depth=5)
```

→ **B**（浅い木 × 多数決）

---

## 18. 演習 ③：特徴量重要度

```python
importances = model.feature_importances_
```

わかること：

- どの特徴が判断に効いたか
- 不要な特徴の発見
- 説明資料に使える

---

## 19. なぜ標準化が不要なのか

- 大小比較しかしない
- 距離計算をしない

→ スケールの影響を受けない

---

## 20. 実務での使い分け指針

- まず試す → ランダムフォレスト
- 説明責任 → 決定木
- 高精度追求 → 勾配ブースティング / NN

---
