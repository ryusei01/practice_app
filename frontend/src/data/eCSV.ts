export const E_CSV = `
question_text,question_type,options,correct_answer,explanation,difficulty,category,subcategory1,subcategory2
"機械学習における教師あり学習の目的として最も適切なのはどれか？",multiple_choice,"入力と正解の対応付けを学習する,クラスタ構造を発見する,次元を削減する,ランダムに重みを設定する","入力と正解の対応付けを学習する","教師あり学習では大量のラベル付きデータを用いて、入力と目的変数の関係をモデルが学習する。分類・回帰問題が典型であり、目的は未知データに対する汎化性能を高めることである。",0.2,機械学習,教師あり学習,基礎
"誤差逆伝播法の役割は何か？",multiple_choice,"重み更新量を計算する,データを正規化する,次元削減を行う,学習率を決定する","重み更新量を計算する","誤差逆伝播法は、ネットワークの出力誤差を各層に逆伝搬し、損失関数の勾配を効率的に求める手法である。これにより深い層まで重み更新が可能になった。",0.3,深層学習,基礎数学,勾配計算
"活性化関数ReLUの利点として正しいものはどれか？",multiple_choice,"勾配消失が起こりにくい,負の値を強調できる,周期性がある,常に滑らかである","勾配消失が起こりにくい","ReLUは0以下を0、0より大きい値はそのまま出力する単純な関数で、深いネットワークでも勾配が残りやすく学習が安定する。特にSigmoidで問題となる勾配消失を緩和する点が重要。",0.2,深層学習,活性化関数,ReLU
"バッチ正規化（Batch Normalization）の主な効果は何か？",multiple_choice,"内部共変量シフトを抑える,学習率を自動調整する,重みを圧縮する,学習データ量を増やす","内部共変量シフトを抑える","バッチ正規化は各ミニバッチ内で平均・分散を揃え、層間の分布変動を抑えることで学習を安定化し収束を早める。深いネットワークの訓練に有効。",0.4,深層学習,正則化,バッチ正規化
"過学習を抑制する手法として最も適切なのはどれか？",multiple_choice,"ドロップアウト,学習率を上げる,層を増やす,データをランダム削除する","ドロップアウト","ドロップアウトは学習時にランダムにユニットを無効化し、ネットワークの依存を減らして汎化性能を高める正則化の一種。",0.3,深層学習,正則化,ドロップアウト
"勾配消失問題が顕著に現れやすいのはどの活性化関数か？",multiple_choice,"Sigmoid,Tanh,ReLU,LeakyReLU","Sigmoid","Sigmoidは出力が0付近で勾配が非常に小さくなる性質を持つため、深いネットワークで勾配が伝わりにくく勾配消失を起こしやすい。",0.4,深層学習,勾配問題,活性化関数
"ニューラルネットワークの重みを初期化する目的として正しいものはどれか？",multiple_choice,"学習を安定させる,学習を遅くする,モデル容量を減らす,推論速度を上げる","学習を安定させる","適切な初期化（XavierやHe）は勾配爆発・消失を避け、ネットワーク学習の初期段階の安定化に重要。",0.3,深層学習,初期化,勾配安定化
"最急降下法の更新式に含まれる学習率の役割は何か？",multiple_choice,"更新幅を調整する,訓練データ量を増やす,層数を決定する,重みを固定する","更新幅を調整する","学習率は勾配方向にどれだけ進むかを決めるパラメータ。大きすぎると発散し、小さすぎると収束が遅くなる。",0.2,深層学習,最適化,学習率
"ニューラルネットワークにおいてEpochとは何を意味するか？",multiple_choice,"訓練データを1周学習すること,重みを初期化する回数,推論回数,ミニバッチのサイズ","訓練データを1周学習すること","Epochは訓練データ全体を何回学習したかを示す指標。多すぎると過学習の原因になることもある。",0.1,機械学習,学習手順,評価
"CNNが画像処理に適している理由として正しいものはどれか？",multiple_choice,"局所受容野を持つ,データをランダム生成できる,重みが不要,画像を圧縮するため","局所受容野を持つ","CNNは周囲の局所パターン（エッジ・角）を捉えられる畳み込み構造を持ち、画像特徴抽出に優れる。",0.3,深層学習,CNN,特徴抽出
"畳み込み層で使われるフィルタ（カーネル）の役割は何か？",multiple_choice,"空間的特徴を抽出する,画像を分割する,ラベルを生成する,分類を直接行う","空間的特徴を抽出する","フィルタは画像の局所領域にスライドし、エッジ・模様などの特徴を抽出する。CNNの基礎要素。",0.2,深層学習,CNN,カーネル
"プーリング層の主な目的はどれか？",multiple_choice,"空間サイズを縮小し特徴を保持する,データを増やす,重みを学習する,画素値を正規化する","空間サイズを縮小し特徴を保持する","MaxPoolingなどは空間サイズを縮小して計算量を削減し、位置ずれに頑健なモデルを作る。",0.2,深層学習,CNN,プーリング
"RNNが扱うのに適したデータはどれか？",multiple_choice,"時系列データ,静止画像,カテゴリデータ,2次元行列","時系列データ","RNNは前の状態を保持できるため、時系列やテキストのように順序依存のデータ処理に適している。",0.3,深層学習,RNN,時系列
"LSTMにおけるForget Gateの役割は何か？",multiple_choice,"過去情報をどの程度保持するか決める,出力値を決める,勾配を増幅する,入力を固定する","過去情報をどの程度保持するか決める","Forget Gateはメモリセルのどの部分を忘れるかを制御し、長期依存の学習を安定化させる。",0.5,深層学習,LSTM,ゲート機構
"Attention機構の本質的な役割は何か？",multiple_choice,"重要な部分に重みを集中させる,学習率を調整する,計算量を減らす,データ前処理を行う","重要な部分に重みを集中させる","Attentionは入力の中で重要な部分に高い重みを与える仕組み。長距離依存の処理に強い。",0.5,深層学習,Attention,Transformer基礎
"TransformerでRNNが不要になる主な理由はどれか？",multiple_choice,"全ての位置間関係をAttentionで直接表現できる,勾配を計算しない,重みが固定されている,画像処理向けだから","全ての位置間関係をAttentionで直接表現できる","Transformerは系列位置の依存をSelf-Attentionで効率的に扱えるため、RNNの逐次処理が不要になる。",0.6,深層学習,Transformer,Self-Attention
"自己注意(Self-Attention)の計算に必要な行列はどれか？",multiple_choice,"Query,Key,Value,Label","Query,Key,Value","Self-AttentionはQ,K,Vを使ってAttention Scoreを算出し、文脈を反映した表現を得る。",0.5,深層学習,Transformer,QKV
"Batchサイズを大きくすると一般的にどうなるか？",multiple_choice,"学習が安定しやすい,常に精度が上がる,学習が必ず早く終わる,過学習が必ず防げる","学習が安定しやすい","大きなバッチは勾配の分散が小さくなり、更新が安定するが、汎化性能が必ず向上するわけではない。",0.4,深層学習,最適化,Batch
"学習率が高すぎる場合に起こりやすい問題はどれか？",multiple_choice,"発散,過学習,勾配消失,データ不足","発散","学習率が高いと損失が最小値を通り過ぎ、更新が発散する可能性が高くなる。",0.3,深層学習,最適化,学習率
"正則化L1の特徴はどれか？",multiple_choice,"重みをスパースにする,重みを滑らかにする,勾配を増幅する,データを増やす","重みをスパースにする","L1正則化は重みを0に近づけ、特徴選択の効果を持つスパース性が特徴。",0.4,機械学習,正則化,L1
"正則化L2の特徴はどれか？",multiple_choice,"重みを小さく保つ,重みを完全に0にする,層を削除する,学習率を調整する","重みを小さく保つ","L2正則化は重みの大きさを抑え、過学習を抑制する滑らかなペナルティを与える。",0.3,機械学習,正則化,L2
"混同行列でPositiveを正しく予測した数を示すのはどれか？",multiple_choice,"True Positive,False Positive,True Negative,False Negative","True Positive","TPは実際Positiveかつ予測Positiveの数で、分類性能評価の基本。",0.2,評価指標,混同行列,分類
"Precisionの定義として正しいものはどれか？",multiple_choice,"TP/(TP+FP),TP/(TP+FN),(TP+TN)/(全体),FN/(TP+FN)","TP/(TP+FP)","Precisionは予測Positiveのうち正しく当たった割合。誤爆の少なさを表す。",0.3,評価指標,Precision,分類
"Recallの定義として正しいものはどれか？",multiple_choice,"TP/(TP+FN),TP/(TP+FP),TN/(TN+FP),FP/(TP+FP)","TP/(TP+FN)","Recallは実際Positiveのうち正しく検出できた割合。取りこぼしの少なさを示す。",0.3,評価指標,Recall,分類
"F1スコアはどのような値か？",multiple_choice,"PrecisionとRecallの調和平均,単純平均,最大値,最小値","PrecisionとRecallの調和平均","PrecisionとRecallのバランスを表す指標で、どちらかが低いとF1も低くなる。",0.4,評価指標,F1,分類
"ROC曲線の横軸は何か？",multiple_choice,"False Positive Rate,True Positive Rate,Precision,Accuracy","False Positive Rate","ROCは分類器性能を閾値で変化させたときのTPRとFPRの関係を示す。",0.5,評価指標,ROC,AUC
"AUCの値が1に近いほど何を意味するか？",multiple_choice,"分類性能が高い,過学習している,学習率が高い,データが多い","分類性能が高い","AUCはROC曲線の面積で、1に近いほどPositive/Negativeを明確に区別できる。",0.4,評価指標,AUC,分類
"クロスエントロピー損失が低いとは何を示すか？",multiple_choice,"予測分布が正解分布に近い,学習が発散している,データが不足している,正則化が強すぎる","予測分布が正解分布に近い","クロスエントロピーは予測確率分布と正解の差異を測る指標で、値が低いほどモデルが正解を高確率で予測していることを示す。",0.4,深層学習,損失関数,分類
"ソフトマックス関数の役割として正しいものはどれか？",multiple_choice,"確率分布へ変換する,値を二値化する,特徴量を削除する,勾配を0にする","確率分布へ変換する","Softmaxは各出力を0〜1に正規化し、総和を1にすることで確率分布として扱えるようにする。",0.3,深層学習,活性化関数,Softmax
"線形回帰の目的はどれか？",multiple_choice,"連続値を予測する,画像を分類する,文章を生成する,クラスタを見つける","連続値を予測する","線形回帰は入力の線形結合で連続値を予測する古典的なモデル。",0.1,機械学習,回帰,基礎
"ロジスティック回帰が扱う問題はどれか？",multiple_choice,"分類問題,回帰問題,クラスタリング,次元削減","分類問題","ロジスティック回帰はシグモイド関数で二値分類を行うモデル。",0.2,機械学習,分類,ロジスティック回帰
"k-meansクラスタリングの目的はどれか？",multiple_choice,"データをk個のクラスタに分割する,データを分類する,時系列を予測する,ラベルを生成する","データをk個のクラスタに分割する","k-meansはデータを距離の近さでクラスタに分ける非教師あり学習手法。",0.2,機械学習,クラスタリング,k-means
"PCAの主な目的はどれか？",multiple_choice,"次元削減,分類性能向上,ラベル生成,損失最小化","次元削減","PCAはデータの分散が最大となる方向に射影し次元を削減する。",0.3,機械学習,次元削減,PCA
"正規化処理の目的はどれか？",multiple_choice,"特徴量のスケールを揃える,重みを増やす,データを増やす,損失を増大させる","特徴量のスケールを揃える","スケーリングは勾配の安定化や距離ベース手法の性能向上に重要。",0.2,前処理,正規化,スケーリング
"ミニバッチ学習の利点はどれか？",multiple_choice,"計算効率と安定性の両立,常に最適解が得られる,学習率が不要,重みが固定される","計算効率と安定性の両立","ミニバッチは全データ学習より高速で、1件ずつの学習より安定した勾配が得られる。",0.3,深層学習,学習手法,ミニバッチ
"勾配爆発を防ぐ手法はどれか？",multiple_choice,"Gradient Clipping,ドロップアウト,Softmax,バッチサイズを減らす","Gradient Clipping","勾配が極端に大きくなるのを防ぎ、RNNなどの学習を安定させるために利用される。",0.5,深層学習,安定化,勾配クリッピング
"Early Stoppingの目的は何か？",multiple_choice,"過学習の防止,学習速度の向上,データの圧縮,特徴量の削除","過学習の防止","検証データの損失が増え始めたら学習を止めることで過学習を防ぐ。",0.3,深層学習,正則化,EarlyStopping
"重み共有が行われる層はどれか？",multiple_choice,"畳み込み層,全結合層,RNNセル,Softmax層","畳み込み層","CNNではフィルタを全体で共有し、パラメータ数削減と平行移動への不変性を実現する。",0.4,深層学習,CNN,重み共有
"ResNetが深いネットワークを学習可能にした理由はどれか？",multiple_choice,"残差接続により勾配消失を緩和した,層数を減らした,データを増やした,正則化を強化した","残差接続により勾配消失を緩和した","ResNetは恒等写像を加える残差接続により、深い層でも勾配が伝わりやすくなった。",0.6,深層学習,CNN,ResNet
"VAEの潜在変数に関する特徴として適切なのはどれか？",multiple_choice,"確率分布として学習される,固定値である,ラベルと一致する,外部から入力する","確率分布として学習される","VAEでは潜在空間が正規分布に従う確率的表現として学習され、生成の滑らかさが保たれる。",0.7,生成モデル,VAE,潜在空間
"GANのGeneratorの目的はどれか？",multiple_choice,"本物と見分けがつかないデータを生成する,本物か判定する,損失を最小化する,潜在空間をクラスタ化する","本物と見分けがつかないデータを生成する","GANではGeneratorが偽物を生成し、Discriminatorが真偽を判定して競い合う。",0.6,生成モデル,GAN,Generator
"GANが不安定になる原因として代表的なのはどれか？",multiple_choice,"Discriminatorの学習が強すぎる,Generatorが常に勝つ,データが重複しない,ラベルが多すぎる","Discriminatorの学習が強すぎる","判別器が強すぎると生成器が勾配を得られず、学習が進まない。",0.7,生成モデル,GAN,安定性
"Diffusion Modelの前向き過程の役割はどれか？",multiple_choice,"入力に徐々にノイズを加える,ノイズを除去する,潜在空間を圧縮する,分類を補助する","入力に徐々にノイズを加える","Diffusionでは前向き過程でデータをノイズ化し、逆過程で生成する。これが生成の安定性を高める。",0.8,生成モデル,Diffusion,前向き過程
"強化学習でエージェントが最大化すべきものはどれか？",multiple_choice,"累積報酬,精度,誤差,損失関数","累積報酬","強化学習では行動により得られる報酬を最大化する方針を学習する。",0.4,強化学習,MDP,報酬
"Q-Learningの更新式で最大化する対象はどれか？",multiple_choice,"次状態の最大Q値,現在の報酬,損失関数,特徴量の数","次状態の最大Q値","Q-Learningは貪欲に次状態の最大Q値を利用して更新するオフポリシー手法。",0.5,強化学習,Q-Learning,価値関数
"DQNが安定化に利用する仕組みはどれか？",multiple_choice,"ターゲットネットワーク,ドロップアウト,Softmax,正則化","ターゲットネットワーク","DQNは学習対象と更新対象のネットワークを分離し、学習の発散を防ぐ。",0.6,強化学習,DQN,安定性
"モデルの汎化性能を評価するために必要なデータはどれか？",multiple_choice,"テストデータ,学習データ,未加工データ,データ拡張データ","テストデータ","テストデータは学習に使わない未知データで、汎化能力の評価に用いられる。",0.2,評価,汎化性能,テスト
"交差検証（Cross Validation）の目的はどれか？",multiple_choice,"汎化性能の安定した評価,学習速度の向上,特徴量削除,正則化の強化","汎化性能の安定した評価","データを分割して複数回学習・評価することで偏りを抑えた性能測定が可能。",0.3,評価,CV,汎化性能
"ハイパーパラメータに該当するものはどれか？",multiple_choice,"学習率,重み,バイアス,活性化後の出力","学習率","ハイパーパラメータは学習過程の外側で人間が設定する値で、学習率・Batchサイズ・層数などが該当する。一方、重みやバイアスは訓練で更新されるパラメータである。",0.3,機械学習,ハイパーパラメータ,基礎
"One-hot表現の欠点として正しいものはどれか？",multiple_choice,"次元が増加しやすい,連続性が高すぎる,勾配が計算できない,分類ができない","次元が増加しやすい","One-hotはカテゴリ数だけ次元が増加し、スパースで距離情報を持たないため、大規模語彙で効率が悪い。",0.3,NLP,前処理,符号化
"Word2VecのSkip-gramモデルの目的はどれか？",multiple_choice,"中心語から周辺語を予測する,周辺語から中心語を予測する,文章を要約する,翻訳を行う","中心語から周辺語を予測する","Skip-gramは入力語から文脈語を予測するモデルで、単語の意味分布を学習できる。",0.4,NLP,埋め込み,Word2Vec
"自己回帰モデル(AR)が用いられるのはどのようなデータか？",multiple_choice,"時系列データ,画像データ,カテゴリデータ,グラフデータ","時系列データ","ARモデルは過去値から現在値を予測する時系列モデルで、時系列依存が前提となる。",0.3,時系列,AR,予測
"標準化（Standardization）の結果として正しいものはどれか？",multiple_choice,"平均0・分散1に近づく,平均1に近づく,最大値が1になる,最小値が0になる","平均0・分散1に近づく","標準化は平均を0、標準偏差を1に調整し、勾配計算や最適化を安定させる。",0.2,前処理,標準化,スケーリング
"活性化関数Tanhの特徴として正しいものはどれか？",multiple_choice,"出力が-1から1になる,0から1になる,線形である,勾配が常に一定","出力が-1から1になる","Tanhはゼロ中心で学習が安定しやすいが、勾配消失はSigmoidほどではないが起こりうる。",0.3,深層学習,活性化関数,Tanh
"バニラRNNが長期依存を学習しにくい原因はどれか？",multiple_choice,"勾配消失が発生しやすい,計算量が大きすぎる,重みが共有されない,畳み込みを使わない","勾配消失が発生しやすい","長い系列では勾配が指数的に減少し、過去の情報が伝わらず長期依存を学習しにくくなる。",0.5,深層学習,RNN,勾配問題
"GRUの利点として正しいものはどれか？",multiple_choice,"LSTMよりパラメータが少ない,必ず高精度になる,画像処理に向いている,Attentionを含む","LSTMよりパラメータが少ない","GRUはゲート構造が簡略化されており、LSTMより軽量で学習が速いことが多い。",0.5,深層学習,GRU,ゲート
"BERTが双方向文脈を扱える理由はどれか？",multiple_choice,"Masked Language Modelを使う,単語を逆順に並べる,画像を入力とする,系列を短縮する","Masked Language Modelを使う","BERTは単語の一部をマスクし、前後文脈から推論するMLMにより双方向依存関係を学習できる。",0.6,NLP,BERT,MLM
"教師なし学習に属する手法はどれか？",multiple_choice,"クラスタリング,ロジスティック回帰,線形回帰,決定木","クラスタリング","教師なし学習はラベルなしデータ構造を探索する手法で、クラスタリングや次元削減が該当する。",0.2,機械学習,教師なし学習,クラスタリング
"KNNの特徴として正しいものはどれか？",multiple_choice,"予測時に全データを参照する,学習時に重みを更新する,線形モデルである,ニューラルネットと同じ構造","予測時に全データを参照する","KNNは学習不要で、予測時に最近傍データを探索するためコストが高い。",0.3,機械学習,KNN,距離計算
"アンサンブル学習の例として適切なものはどれか？",multiple_choice,"ランダムフォレスト,単一の線形回帰,単一のCNN,単一のSVM","ランダムフォレスト","アンサンブルは複数モデルを組み合わせて性能向上を図る手法で、ランダムフォレストが代表的。",0.4,機械学習,アンサンブル,ランダムフォレスト
"Boostingの特徴はどれか？",multiple_choice,"弱学習器を順に強化する,複数のモデルを並列に学習する,教師なしで学習する,重み更新を行わない","弱学習器を順に強化する","Boostingは誤分類の重みを調整しながら学習を続け、逐次的にモデルを強化する。",0.5,機械学習,アンサンブル,Boosting
"勾配ブースティングの利点はどれか？",multiple_choice,"高い表現力,重み不要,計算が常に高速,過学習しない","高い表現力","勾配ブースティングは複雑な関係を捉え、単一モデルより高精度を達成しやすい。",0.6,機械学習,Boosting,GBDT
"勾配降下法が局所最適に陥る原因はどれか？",multiple_choice,"非凸問題における複雑な損失地形,データが少ない,活性化関数が一定,学習率が0","非凸問題における複雑な損失地形","深層学習の損失関数は非凸で局所最適や鞍点が多数存在するため、解が最適でない場合がある。",0.6,深層学習,最適化,非凸最適化
"Adamの特徴はどれか？",multiple_choice,"モーメントと適応的学習率を用いる,重みを固定する,勾配を正規化する,常に最適解が得られる","モーメントと適応的学習率を用いる","Adamはモーメント推定と学習率調整により収束を速め、広く使われる最適化手法となっている。",0.5,深層学習,最適化,Adam
"学習率スケジューラの目的はどれか？",multiple_choice,"学習の安定化と高速化,バッチサイズの削減,モデル容量を減らす,重みを固定する","学習の安定化と高速化","学習率を時間経過で小さくすることにより、初期の大きな探索と後半の微調整の両立が可能になる。",0.4,深層学習,最適化,LRスケジューラ
"Dropoutの副作用として起こり得るものはどれか？",multiple_choice,"学習が不安定になることがある,過学習が必ず起きる,勾配が計算できない,モデルが壊れる","学習が不安定になることがある","Dropoutはランダムにユニットを無効化するため、短期的には勾配が揺らぎ学習が不安定になることがある。",0.5,深層学習,正則化,Dropout
"データ拡張(Data Augmentation)の目的はどれか？",multiple_choice,"過学習を防ぐ,推論速度を上げる,損失を増やす,モデルを単純化する","過学習を防ぐ","画像回転・反転などでデータ多様性を増し、モデルの汎化性能を高める。",0.3,前処理,データ拡張,画像
"ラベルスムージングの目的はどれか？",multiple_choice,"過度な自信を防ぐ,モデル容量を減らす,勾配を増やす,標準化を行う","過度な自信を防ぐ","ラベルを1でなく0.9程度にすることで、モデルの過信を防ぎ汎化性能を向上させる。",0.6,深層学習,正則化,ラベルスムージング
"CNNのフィルタ数を増やすと一般的に何が増えるか？",multiple_choice,"表現能力,推論速度,データ数,バッチサイズ","表現能力","フィルタ数は抽出される特徴の種類に直結し、増やすほど複雑な特徴が学習できる。",0.4,深層学習,CNN,容量
"MobileNetが軽量化を実現できる要因はどれか？",multiple_choice,"Depthwise Separable Convolution,Attention層,再帰構造,ポリノミアル回帰","Depthwise Separable Convolution","畳み込みを空間方向とチャネル方向に分離し計算量を大幅削減している。",0.7,深層学習,CNN,MobileNet
"転移学習の利点として正しいものはどれか？",multiple_choice,"少ないデータでも高精度を得やすい,必ず高速推論になる,モデルが常に縮小される,正則化が不要になる","少ないデータでも高精度を得やすい","大規模データで学習したモデルを再利用することで、少量データでも性能向上が期待できる。",0.4,深層学習,転移学習,特徴抽出
"ファインチューニングで特に重要な設定はどれか？",multiple_choice,"学習率,バッチサイズ,重み初期値,正則化係数","学習率","学習率が高すぎると事前学習済み重みが破壊されるため、通常より小さく設定する必要がある。",0.5,深層学習,転移学習,ファインチューニング
"オートエンコーダの潜在空間が担う役割はどれか？",multiple_choice,"入力の圧縮表現,分類ラベルの生成,損失計算の最適化,勾配の保存","入力の圧縮表現","オートエンコーダでは中間層に低次元の潜在表現を持ち、特徴抽出やノイズ除去に使われる。",0.4,深層学習,オートエンコーダ,潜在表現
"オートエンコーダのDecoderの役割はどれか？",multiple_choice,"入力を復元する,特徴量を削除する,分類を行う,ラベルを生成する","入力を復元する","潜在表現から元のデータを復元し、再構成誤差を最小化することが目的。",0.3,深層学習,オートエンコーダ,Decoder
"異常検知でオートエンコーダが利用される理由はどれか？",multiple_choice,"異常データの再構成誤差が大きくなりやすい,異常データを生成できる,確率分布が得られる,系列長が短くなる","異常データの再構成誤差が大きくなりやすい","訓練データが正常のみの場合、異常は再構成困難なため誤差を指標に検知できる。",0.6,異常検知,オートエンコーダ,再構成誤差
"画像でのData Augmentation手法として正しいものはどれか？",multiple_choice,"回転,線形回帰,重み共有,ソフトマックス","回転","回転・平行移動・反転などが画像拡張の基本手法。",0.2,前処理,データ拡張,画像
"自然言語処理での前処理として適切なものはどれか？",multiple_choice,"トークナイズ,プーリング,畳み込み,微分","トークナイズ","文章を単語やサブワード単位に分割することで機械学習モデルが扱いやすくする。",0.3,NLP,前処理,トークナイズ
"サブワード分割手法として一般的なものはどれか？",multiple_choice,"BPE,プーリング,線形補間,勾配クリッピング","BPE","BPEは文字の頻度に基づくマージでサブワードを作り、未知語問題に強い。",0.5,NLP,トークナイズ,BPE
"Transformerで位置情報を扱う仕組みはどれか？",multiple_choice,"Positional Encoding,Attention Mask,Layer Norm,Pooling","Positional Encoding","Transformerは順序を持たないため、位置情報を付加するために正弦波などのPositional Encodingを使用する。",0.6,深層学習,Transformer,PositionalEncoding
"Layer Normalizationの目的はどれか？",multiple_choice,"特徴量方向で正規化する,ミニバッチ方向で正規化する,重み共有を行う,系列長を削減する","特徴量方向で正規化する","LayerNormはTransformerで一般的で、バッチサイズによらず安定した正規化が可能。",0.6,深層学習,正規化,LayerNorm
"AttentionにおけるScaled Dot-Productでスケールを行う理由はどれか？",multiple_choice,"勾配が極端に大きくなるのを防ぐ,Attentionを強める,系列長を短くする,計算を高速化する","勾配が極端に大きくなるのを防ぐ","内積が大きくなりすぎるとSoftmaxが飽和し勾配が消失するため、スケーリングで安定化させる。",0.7,深層学習,Attention,安定化
"Self-Attentionが計算量的に不利になるのはどのような場合か？",multiple_choice,"系列長が長いとき,画像が小さいとき,特徴量が少ないとき,バッチサイズが小さいとき","系列長が長いとき","Self-AttentionはO(n^2)で伸びるため系列長が長いほど計算コストが急増する。",0.6,深層学習,Attention,計算量
"グラフニューラルネットワーク(GNN)の目的はどれか？",multiple_choice,"グラフ構造を持つデータの学習,画像ノイズ除去,音声認識,翻訳の高速化","グラフ構造を持つデータの学習","GNNはノード・エッジの関係を保持したまま情報伝播を行う。",0.7,深層学習,GNN,グラフ
"メモリセルを用いて長期依存を扱う構造はどれか？",multiple_choice,"LSTM,MLP,CNN,AE","LSTM","LSTMは勾配消失を抑え、長期依存性を保持するためのメモリセル構造を持つ。",0.4,深層学習,LSTM,長期依存
"Self-Supervised Learningの例として正しいものはどれか？",multiple_choice,"BERTのMLM,線形回帰,K-means,決定木","BERTのMLM","自己教師あり学習は人工的にタスクを作り、ラベルなしデータで学習する方法である。",0.6,機械学習,自己教師あり学習,MLM
"自己教師あり学習が有効になる理由として正しいものはどれか？",multiple_choice,"大量のラベルなしデータを活用できる,モデル容量を減らせる,計算コストが不要,常に教師ありより高精度になる","大量のラベルなしデータを活用できる","自己教師あり学習はラベル不要で大量の未ラベルデータを活用できるため、特に自然言語・画像分野で強力である。",0.6,機械学習,自己教師あり学習,ラベルなし学習
"Contrastive Learningの目的として正しいものはどれか？",multiple_choice,"類似サンプルを近づけ非類似を離す,重みを固定する,分類境界を作る,ランダムにフィルタを更新する","類似サンプルを近づけ非類似を離す","コントラスト学習は表現空間で正例を近づけ負例を離し、強力な表現学習を行う手法である。",0.7,機械学習,自己教師あり学習,ContrastiveLearning
"SimCLRで用いられる重要な要素はどれか？",multiple_choice,"データ拡張,重み共有禁止,線形モデル,決定木","データ拡張","SimCLRは異なる拡張を施した同一画像を正例ペアとして扱いコントラスト学習する。",0.7,深層学習,自己教師あり学習,SimCLR
"クラスタリング結果を評価するためにラベル不要で使える指標はどれか？",multiple_choice,"シルエット係数,F1スコア,Accuracy,ROC","シルエット係数","クラスタリングは教師なしであるため、シルエット係数などの内部評価指標を使う。",0.5,機械学習,クラスタリング,評価
"K-meansの初期値によって変化しやすいものはどれか？",multiple_choice,"クラスタの結果,損失関数の定義,データの分布,系列長","クラスタの結果","K-meansは初期中心点に依存し、局所解に陥りやすいため初期値選択が重要となる。",0.5,機械学習,クラスタリング,k-means
"次元削減手法t-SNEの特徴として正しいものはどれか？",multiple_choice,"局所構造を重視する,線形変換である,常に高速である,クラスタを直接生成する","局所構造を重視する","t-SNEは高次元データの局所的な類似関係を保ちながら低次元に可視化する非線形手法。",0.6,機械学習,次元削減,t-SNE
"UMAPがt-SNEに比べて優れている点として正しいものはどれか？",multiple_choice,"高速で大規模データに強い,常に高精度,損失が不要,教師ありである","高速で大規模データに強い","UMAPはt-SNEより計算が高速で、大規模データの可視化に適している。",0.6,機械学習,次元削減,UMAP
"決定木の欠点として正しいものはどれか？",multiple_choice,"過学習しやすい,線形性が強すぎる,データ拡張が必須,計算できない場合が多い","過学習しやすい","決定木は分割を繰り返すため複雑になりやすく、単体では過学習を起こしやすい。",0.4,機械学習,決定木,欠点
"ランダムフォレストが過学習に強い理由はどれか？",multiple_choice,"複数の決定木をランダムに学習する,常に深さ1に制限する,データを削除する,重みを共有する","複数の決定木をランダムに学習する","ランダムサンプリングによりモデル間の相関を下げ、過学習を抑制できる。",0.5,機械学習,アンサンブル,ランダムフォレスト
"XGBoostが高速に学習できる理由はどれか？",multiple_choice,"近似木構造の導入,全データを完全に学習する,ニューラルネットを使用する,活性化関数が不要","近似木構造の導入","XGBoostは分割候補を近似し、高速かつ高性能な決定木ブースティングを可能にしている。",0.7,機械学習,Boosting,XGBoost
"LightGBMの特徴として正しいものはどれか？",multiple_choice,"Leaf-wise成長戦略,バッチ正規化を使う,勾配消失を防ぐ,画像処理に向く","Leaf-wise成長戦略","LightGBMは葉ノードを優先して成長させる戦略で高精度・高速を両立している。",0.7,機械学習,Boosting,LightGBM
"ハイパーパラメータ探索でGrid Searchの欠点はどれか？",multiple_choice,"探索コストが高い,最適解が得られない,ランダム性が強い,学習率が不要","探索コストが高い","Grid Searchは組み合わせを総当たりするため計算コストが高く、大規模モデルでは非効率である。",0.5,機械学習,ハイパーパラメータ探索,GridSearch
"ランダムサーチがGrid Searchより効率的な理由はどれか？",multiple_choice,"重要な次元に多く時間を割ける,常に最適解に到達する,次元削減ができる,データ拡張が行える","重要な次元に多く時間を割ける","ランダムサーチは特定の次元に偏りやすいGridSearchより探索効率が高いことが多い。",0.6,機械学習,ハイパーパラメータ探索,RandomSearch
"ベイズ最適化の利点はどれか？",multiple_choice,"探索を効率化し少ない試行で良い値を見つける,必ず最適解が得られる,勾配不要,重みを削除できる","探索を効率化し少ない試行で良い値を見つける","ベイズ最適化は獲得関数により有望な領域に集中し、試行回数を大幅に削減する。",0.7,機械学習,ハイパーパラメータ探索,ベイズ最適化
"学習データとテストデータを適切に分割する目的はどれか？",multiple_choice,"汎化性能の正確な評価,学習速度の向上,損失を小さくする,特徴量選択を行う","汎化性能の正確な評価","テストデータは未知データとして扱い、過学習を検出しモデル性能を正しく評価する。",0.3,評価,データ分割,汎化性能
"機械学習におけるバイアスとは何か？",multiple_choice,"モデルの仮定による誤差,データの個数,勾配の大きさ,重み行列の形状","モデルの仮定による誤差","モデルが単純すぎる場合に発生する誤差をバイアスと呼ぶ。",0.4,機械学習,誤差分解,バイアス
"機械学習におけるバリアンスとは何か？",multiple_choice,"データ依存性による誤差,学習率の大きさ,勾配不足,特徴量数","データ依存性による誤差","モデルが複雑すぎてデータに過適合すると、データ依存性が強くなり誤差が増える。これがバリアンスである。",0.4,機械学習,誤差分解,バリアンス
"バイアス-バリアンスのトレードオフとは何か？",multiple_choice,"モデルの複雑さに応じて誤差が増減する,データ数が増えると誤差が増える,損失が常に減少する,学習率を増やすと精度が上がる","モデルの複雑さに応じて誤差が増減する","モデルが単純すぎるとバイアスが大きく、複雑すぎるとバリアンスが大きくなる関係を指す。",0.5,機械学習,誤差分解,トレードオフ
"過学習の典型的な兆候はどれか？",multiple_choice,"訓練精度が高いがテスト精度が低い,訓練データが不足している,重みが0になる,学習が進まない","訓練精度が高いがテスト精度が低い","訓練データに特化した学習が進むと未知データで性能が下がるため過学習が検出できる。",0.3,機械学習,過学習,評価
"正則化が過学習を抑制する仕組みとして正しいものはどれか？",multiple_choice,"重みの自由度を減らす,データ数を増やす,学習率を上げる,系列長を短くする","重みの自由度を減らす","正則化はモデルの自由度を制限し、複雑なパターンに過適合するのを防ぐ。",0.4,機械学習,正則化,過学習防止
"データのシャッフルが重要な理由はどれか？",multiple_choice,"学習順序の偏りを防ぐ,推論速度を上げる,損失を減らす,モデル容量を減らす","学習順序の偏りを防ぐ","データが順番に偏ると勾配が歪み学習が不安定になるためランダム化が重要。",0.2,前処理,データ準備,シャッフル
"学習率が小さすぎる場合に起こる現象はどれか？",multiple_choice,"収束が遅い,発散する,勾配が0になる,系列長が伸びる","収束が遅い","過小な学習率では損失最小値に到達するまでに多くのステップが必要になる。",0.3,深層学習,最適化,学習率
"Min-Maxスケーリングの特徴はどれか？",multiple_choice,"0〜1に正規化する,平均0へ変換する,分散を1にする,外れ値に強い","0〜1に正規化する","Min-Maxは最大・最小値を基に0〜1へスケーリングするが外れ値に弱い点がある。",0.3,前処理,スケーリング,MinMax
"分散が大きい特徴量があると何が起こりやすいか？",multiple_choice,"距離計算が歪む,モデルが削除される,勾配が不要になる,推論が停止する","距離計算が歪む","スケールが揃っていないと距離ベース手法で一部特徴が支配的になり不安定になる。",0.3,前処理,特徴量スケール,問題点
"外れ値処理でWinsorizationを行う目的はどれか？",multiple_choice,"外れ値の影響を抑える,外れ値を削除する,特徴を増やす,系列を短縮する","外れ値の影響を抑える","Winsorizationは外れ値を特定パーセンタイルに置き換え、影響を抑制する手法。",0.5,前処理,外れ値処理,Winsorization
"欠損値処理で最も単純な方法はどれか？",multiple_choice,"平均値補完,線形補完,クラスタリング,勾配計算","平均値補完","最も簡易的な方法だがデータの分布を歪める可能性もある。",0.2,前処理,欠損値,補完
"欠損値を削除する際の注意点はどれか？",multiple_choice,"データ量が減少する,損失が増える,勾配が変わる,特徴量が隠れる","データ量が減少する","行削除は簡便だがデータが失われるため慎重に選択する必要がある。",0.3,前処理,欠損値,削除
"異常検知でOne-class SVMが使われる理由はどれか？",multiple_choice,"正常データのみで境界を学習できる,分類問題に強い,ラベルが多いほど精度向上,必ず高精度","正常データのみで境界を学習できる","One-class SVMは教師なしで正常データの外側を異常領域とする用途で使われる。",0.6,異常検知,SVM,OneClass
"時系列予測で移動平均(MA)が捉えるものはどれか？",multiple_choice,"過去誤差の平均,未来値,系列長,損失値","過去誤差の平均","MAモデルは誤差項の線形結合で時系列を表し、ARとは異なる性質を持つ。",0.5,時系列,MA,予測
"ARIMAのI(差分)が担う役割はどれか？",multiple_choice,"非定常性を取り除く,勾配計算,画像圧縮,クラスタ生成","非定常性を取り除く","Iは差分を取りトレンドを除去し、時系列を定常化する。",0.6,時系列,ARIMA,差分
"シャープレシオの用途として適切なのはどれか？",multiple_choice,"リスクとリターンのバランスを測る,画像の鮮明度,分類性能,誤差計算","リスクとリターンのバランスを測る","シャープレシオは金融で利用され、リスク当たりの収益性を評価する。",0.5,時系列,金融指標,リスク評価
"クロスエントロピー損失が分類に向く理由はどれか？",multiple_choice,"確率分布の差を直接測定できる,常に低コスト,計算不要,損失が整数値","確率分布の差を直接測定できる","分類問題は出力が確率であり、正解との分布距離を測るクロスエントロピーが適している。",0.4,深層学習,損失関数,分類
"ヒンジ損失が利用される代表的モデルはどれか？",multiple_choice,"SVM,MLP,CNN,RNN","SVM","ヒンジ損失はマージン最大化を目的とするSVMと相性がよい。",0.5,機械学習,損失関数,ヒンジ損失
"多層パーセプトロン(MLP)の本質的な特徴はどれか？",multiple_choice,"非線形変換を重ねる,畳み込みを行う,系列依存を扱う,Attentionを使う","非線形変換を重ねる","MLPは非線形活性化関数による多段変換で複雑な関数を近似する。",0.3,深層学習,MLP,基礎
"ユニバーサル近似定理が示すものはどれか？",multiple_choice,"MLPは任意の関数を近似できる,最適化が不要になる,必ず高精度になる,勾配計算が不要","MLPは任意の関数を近似できる","十分なニューロンと非線形性があればMLPは任意の連続関数を近似可能であることを示す。",0.6,深層学習,MLP,理論
"ニューラルネットワークで線形分離不可能な問題を扱える理由はどれか？",multiple_choice,"非線形活性化関数を用いる,層数が多い,重みが多い,データが多い","非線形活性化関数を用いる","非線形活性化関数を挟むことで、線形モデルでは表現できない複雑な境界を学習できる。",0.4,深層学習,理論,非線形性
"畳み込みニューラルネットワークでストライドを大きくするとどうなるか？",multiple_choice,"出力サイズが小さくなる,精度が必ず上がる,計算量が増える,特徴が増える","出力サイズが小さくなる","ストライドはフィルタの移動幅で、大きくすると空間解像度が下がる。",0.3,深層学習,CNN,ストライド
"Zero Paddingを行う主な理由はどれか？",multiple_choice,"出力サイズを維持する,学習率を下げる,重みを初期化する,ノイズを除去する","出力サイズを維持する","パディングにより畳み込み後の特徴マップサイズを制御できる。",0.3,深層学習,CNN,パディング
"Dilated Convolutionの利点はどれか？",multiple_choice,"受容野を広げつつ計算量を抑える,必ず高精度になる,パラメータを増やす,勾配消失を防ぐ","受容野を広げつつ計算量を抑える","ダイレーションにより広い文脈を少ないパラメータで捉えられる。",0.6,深層学習,CNN,DilatedConv
"全結合層がCNNの後段に用いられる理由はどれか？",multiple_choice,"抽出特徴を統合し判断する,画像サイズを縮小する,ノイズを除去する,特徴を増幅する","抽出特徴を統合し判断する","畳み込みで得た特徴を最終的な分類・回帰に結びつける役割を担う。",0.3,深層学習,CNN,全結合
"Global Average Poolingの利点はどれか？",multiple_choice,"パラメータ削減と過学習抑制,精度が必ず上がる,系列処理が可能,Attentionを代替する","パラメータ削減と過学習抑制","全結合層を減らしモデルを軽量化できる。",0.5,深層学習,CNN,GAP
"Attention Maskの役割として正しいものはどれか？",multiple_choice,"不要な位置へのAttentionを無効化する,学習率を制御する,勾配を遮断する,特徴量を削除する","不要な位置へのAttentionを無効化する","パディング部分や未来情報への参照を防ぐために用いられる。",0.5,深層学習,Transformer,Mask
"Encoder-Decoder構造の主な用途はどれか？",multiple_choice,"系列変換タスク,画像分類,クラスタリング,異常検知","系列変換タスク","翻訳や要約など、入力系列を別の系列へ変換する問題に用いられる。",0.4,NLP,EncoderDecoder,seq2seq
"Seq2SeqモデルでTeacher Forcingを用いる理由はどれか？",multiple_choice,"学習を安定させる,推論を高速化する,モデル容量を減らす,誤差を0にする","学習を安定させる","正解トークンを次入力として使い、誤差蓄積を防ぐ。",0.6,NLP,Seq2Seq,TeacherForcing
"Exposure Biasが発生する理由はどれか？",multiple_choice,"学習と推論で入力分布が異なる,データが少ない,モデルが浅い,学習率が高い","学習と推論で入力分布が異なる","Teacher Forcingにより学習時と推論時の条件が異なることで生じる。",0.7,NLP,Seq2Seq,ExposureBias
"Beam Searchの目的はどれか？",multiple_choice,"系列生成の品質向上,学習の高速化,データ圧縮,正則化","系列生成の品質向上","複数候補を保持しながら最適系列を探索する。",0.5,NLP,生成,BeamSearch
"Greedy Decodingの欠点はどれか？",multiple_choice,"局所最適に陥りやすい,計算量が大きい,実装が難しい,推論できない","局所最適に陥りやすい","毎ステップ最良を選ぶため全体最適を逃すことがある。",0.4,NLP,生成,Decoding
"BLEUスコアが評価するものはどれか？",multiple_choice,"生成文と参照文のn-gram一致度,分類精度,確率分布の差,文法正確性","生成文と参照文のn-gram一致度","機械翻訳の代表的自動評価指標。",0.5,NLP,評価,BLEU
"ROUGEが主に使われるタスクはどれか？",multiple_choice,"要約,画像分類,異常検知,回帰","要約","参照要約との重なりで評価する。",0.4,NLP,評価,ROUGE
"Perplexityが低いモデルは何を意味するか？",multiple_choice,"言語モデルとして予測が良い,必ず翻訳精度が高い,過学習している,系列が短い","言語モデルとして予測が良い","次単語予測の不確実性が低いことを示す。",0.5,NLP,評価,Perplexity
"Embedding層の役割として正しいものはどれか？",multiple_choice,"離散トークンを連続ベクトルに変換する,分類を行う,損失を計算する,系列長を短縮する","離散トークンを連続ベクトルに変換する","意味的類似性を反映した表現を得る。",0.3,NLP,埋め込み,Embedding
"埋め込み次元を大きくしすぎると起こり得る問題はどれか？",multiple_choice,"過学習,必ず高精度,計算不能,勾配消失","過学習","パラメータ増加により汎化性能が低下する可能性がある。",0.5,NLP,埋め込み,次元数
"Negative Samplingの目的はどれか？",multiple_choice,"計算量削減,精度低下,勾配増幅,系列短縮","計算量削減","全語彙Softmaxの代替として効率化する。",0.6,NLP,Word2Vec,NegativeSampling
"ELMoの特徴として正しいものはどれか？",multiple_choice,"文脈依存の埋め込み,固定ベクトル,Attention非使用,単方向","文脈依存の埋め込み","同一単語でも文脈により表現が変わる。",0.6,NLP,埋め込み,ELMo
"GPT系モデルがDecoderのみで構成される理由はどれか？",multiple_choice,"自己回帰生成に特化するため,計算不要,Encoderが不要,分類専用","自己回帰生成に特化するため","過去トークンのみ参照して次を生成する設計。",0.6,NLP,Transformer,GPT
"Causal Maskの目的はどれか？",multiple_choice,"未来トークン参照を防ぐ,学習率調整,正規化,データ拡張","未来トークン参照を防ぐ","自己回帰性を保つために使用。",0.5,NLP,Transformer,CausalMask
"Fine-tuning時に全層を更新するリスクはどれか？",multiple_choice,"事前学習知識の破壊,計算不能,必ず精度向上,推論不可","事前学習知識の破壊","Catastrophic Forgettingが起こり得る。",0.6,深層学習,転移学習,破壊
"Catastrophic Forgettingとは何か？",multiple_choice,"新タスク学習で旧知識を失う,過学習,勾配消失,精度向上","新タスク学習で旧知識を失う","連続学習で問題となる現象。",0.6,深層学習,継続学習,忘却
"Multi-Task Learningの利点はどれか？",multiple_choice,"表現共有による汎化向上,計算不要,必ず高精度,学習不要","表現共有による汎化向上","関連タスクを同時学習し性能を高める。",0.5,深層学習,学習戦略,MultiTask
"Knowledge Distillationの目的はどれか？",multiple_choice,"軽量モデルへ知識転移,モデル巨大化,精度低下,学習不要","軽量モデルへ知識転移","TeacherからStudentへ知識を移す。",0.6,深層学習,蒸留,KnowledgeDistillation
"蒸留でSoft Targetを使う理由はどれか？",multiple_choice,"クラス間関係を伝える,勾配消失防止,データ削減,学習率調整","クラス間関係を伝える","Hard Labelより情報量が多い。",0.6,深層学習,蒸留,SoftTarget
"Federated Learningの目的はどれか？",multiple_choice,"データを共有せず学習する,学習を高速化,モデル縮小,分類精度低下","データを共有せず学習する","プライバシー保護が主目的。",0.6,機械学習,分散学習,Federated
"差分プライバシーが保証するものはどれか？",multiple_choice,"個人情報の影響を限定する,完全匿名化,精度最大化,学習不要","個人情報の影響を限定する","単一データの有無が結果に与える影響を抑える。",0.7,AI倫理,プライバシー,差分プライバシー
"モデル説明手法LIMEの特徴はどれか？",multiple_choice,"局所的説明を行う,全体構造を可視化,勾配不要,決定木化","局所的説明を行う","特定予測の周辺を線形近似する。",0.6,解釈性,LIME,説明
"SHAP値が基づく理論はどれか？",multiple_choice,"協力ゲーム理論,確率論,情報理論,線形代数","協力ゲーム理論","特徴量貢献度を公平に分配する考え方。",0.6,解釈性,SHAP,理論
"Explainable AIが重要視される理由はどれか？",multiple_choice,"信頼性と説明責任,精度向上のみ,学習不要,高速推論","信頼性と説明責任","社会実装で不可欠。",0.5,AI倫理,説明可能性,XAI
"AIモデルのバイアス問題の原因として適切なのはどれか？",multiple_choice,"訓練データの偏り,活性化関数,学習率,GPU性能","訓練データの偏り","データ分布が公平でないと出力も偏る。",0.5,AI倫理,バイアス,公平性
"フェアネス指標Demographic Parityが意味するものはどれか？",multiple_choice,"属性に関わらず同一予測率,精度最大化,誤差最小化,系列一致","属性に関わらず同一予測率","結果の公平性を測る指標。",0.7,AI倫理,公平性,評価
"モデル監視(Monitoring)の目的はどれか？",multiple_choice,"性能劣化検知,学習不要,推論停止,特徴削除","性能劣化検知","データドリフトなどを検知する。",0.4,MLOps,運用,監視
"データドリフトとは何か？",multiple_choice,"入力分布の変化,勾配消失,モデル破壊,推論高速化","入力分布の変化","学習時と運用時のデータ差異。",0.5,MLOps,データドリフト,運用
"コンセプトドリフトの特徴はどれか？",multiple_choice,"入力とラベル関係の変化,入力欠損,データ削除,特徴増加","入力とラベル関係の変化","問題定義自体が変化する。",0.6,MLOps,コンセプトドリフト,運用
"再学習(Retraining)が必要になる状況はどれか？",multiple_choice,"性能低下が観測されたとき,精度が高いとき,データが固定,推論のみ","性能低下が観測されたとき","運用中の品質維持に不可欠。",0.4,MLOps,運用,再学習
"オンライン学習の利点はどれか？",multiple_choice,"逐次データへ適応可能,必ず高精度,過学習防止,学習不要","逐次データへ適応可能","ストリームデータに対応。",0.5,機械学習,学習形態,オンライン学習
"バッチ学習とオンライン学習の違いとして正しいものはどれか？",multiple_choice,"一括更新か逐次更新か,モデル構造,活性化関数,損失関数","一括更新か逐次更新か","バッチ学習は全データでまとめて学習し、オンライン学習は逐次的に更新する。",0.3,機械学習,学習形態,比較
"ミニバッチ学習の利点はどれか？",multiple_choice,"計算効率と安定性の両立,必ず最良解,過学習防止のみ,実装不要","計算効率と安定性の両立","バッチとオンラインの中間的手法。",0.3,機械学習,学習形態,ミニバッチ
"エポックとは何を指すか？",multiple_choice,"全訓練データを一巡すること,1バッチの処理,重み更新回数,学習率","全訓練データを一巡すること","学習回数の単位。",0.2,機械学習,学習過程,エポック
"Early Stoppingの目的はどれか？",multiple_choice,"過学習防止,学習率増加,精度低下,推論高速化","過学習防止","検証誤差を基準に学習を止める。",0.3,機械学習,正則化,EarlyStopping
"学習率が大きすぎると起こりやすい問題はどれか？",multiple_choice,"発散,過学習,計算不能,汎化向上","発散","最適解を飛び越えてしまう。",0.2,機械学習,最適化,学習率
"学習率減衰(Scheduler)の目的はどれか？",multiple_choice,"収束安定化,精度低下,過学習促進,計算量削減","収束安定化","後半で微調整しやすくする。",0.3,機械学習,最適化,スケジューラ
"Cosine Annealingの特徴はどれか？",multiple_choice,"周期的に学習率を下げる,一定値,線形増加,ランダム","周期的に学習率を下げる","局所解脱出にも有効。",0.6,機械学習,最適化,Cosine
"重み初期化が重要な理由はどれか？",multiple_choice,"勾配消失や爆発を防ぐ,精度保証,データ削減,推論高速化","勾配消失や爆発を防ぐ","適切な分散が学習安定性を高める。",0.4,深層学習,初期化,重み
"Xavier初期化が適している活性化関数はどれか？",multiple_choice,"tanh,ReLU,Step,Softmax","tanh","前後層の分散を保つ。",0.5,深層学習,初期化,Xavier
"He初期化が適している活性化関数はどれか？",multiple_choice,"ReLU,tanh,Sigmoid,Softmax","ReLU","ReLUの非対称性を考慮。",0.5,深層学習,初期化,He
"Batch Normalizationの効果として正しいものはどれか？",multiple_choice,"学習安定化と高速化,必ず精度向上,正則化不要,推論不可","学習安定化と高速化","内部共変量シフトを緩和。",0.4,深層学習,正規化,BatchNorm
"Layer Normalizationが主に使われる分野はどれか？",multiple_choice,"NLP,画像分類,音声圧縮,異常検知","NLP","系列長依存を受けにくい。",0.5,深層学習,正規化,LayerNorm
"Group Normalizationの利点はどれか？",multiple_choice,"小バッチでも安定,必ず高精度,計算削減,推論不要","小バッチでも安定","Batch Sizeに依存しない。",0.6,深層学習,正規化,GroupNorm
"Dropoutが過学習を防ぐ理由はどれか？",multiple_choice,"特徴共適応を抑制,精度低下,計算削減,勾配消失防止","特徴共適応を抑制","ランダム無効化により汎化向上。",0.4,深層学習,正則化,Dropout
"L1正則化の特徴はどれか？",multiple_choice,"スパース化,必ず高精度,勾配消失,特徴増加","スパース化","不要特徴の重みを0にしやすい。",0.4,機械学習,正則化,L1
"L2正則化の別名はどれか？",multiple_choice,"Weight Decay,Dropout,Normalization,Clipping","Weight Decay","重みの大きさを抑制。",0.3,機械学習,正則化,L2
"勾配クリッピングの目的はどれか？",multiple_choice,"勾配爆発防止,精度低下,学習停止,正規化","勾配爆発防止","特にRNNで重要。",0.4,深層学習,最適化,Clipping
"RNNで長期依存を学習しにくい理由はどれか？",multiple_choice,"勾配消失,計算量過多,データ不足,構造単純","勾配消失","時間方向に勾配が減衰。",0.4,深層学習,RNN,勾配
"LSTMが長期依存を扱える理由はどれか？",multiple_choice,"ゲート機構,層数増加,Attention,正規化","ゲート機構","情報の保持と忘却を制御。",0.4,深層学習,RNN,LSTM
"GRUの特徴として正しいものはどれか？",multiple_choice,"LSTMより簡素,必ず高精度,計算不能,非再帰","LSTMより簡素","パラメータ数が少ない。",0.4,深層学習,RNN,GRU
"Self-Attentionの計算量の特徴はどれか？",multiple_choice,"系列長の二乗に比例,線形,定数,無関係","系列長の二乗に比例","長文で計算負荷が高い。",0.5,深層学習,Attention,計算量
"Multi-Head Attentionの利点はどれか？",multiple_choice,"複数表現空間を学習,計算不要,勾配削減,正則化","複数表現空間を学習","多様な関係性を捉える。",0.4,深層学習,Attention,MultiHead
"Position Encodingが必要な理由はどれか？",multiple_choice,"順序情報付与,計算削減,精度保証,正規化","順序情報付与","Transformerは順序を持たないため。",0.4,深層学習,Transformer,Position
"BERTが双方向モデルである利点はどれか？",multiple_choice,"文脈理解向上,生成特化,計算削減,軽量化","文脈理解向上","前後文脈を同時に考慮。",0.4,NLP,BERT,双方向
"Masked Language Modelの目的はどれか？",multiple_choice,"文脈理解学習,翻訳,生成高速化,分類","文脈理解学習","BERTの事前学習タスク。",0.4,NLP,BERT,MLM
"Next Sentence Predictionの役割はどれか？",multiple_choice,"文関係理解,単語生成,分類高速化,正規化","文関係理解","文レベルの意味を学習。",0.4,NLP,BERT,NSP
"Fine-tuning時に層ごとに学習率を変える理由はどれか？",multiple_choice,"事前学習知識保持,精度低下,学習停止,推論高速化","事前学習知識保持","下層は小さく更新。",0.6,深層学習,転移学習,学習率
"Prompt Learningの特徴はどれか？",multiple_choice,"モデル重みを固定,再学習不要,必ず高精度,軽量","モデル重みを固定","入力設計でタスク適応。",0.6,NLP,Prompt,学習
"Few-shot Learningの条件として正しいものはどれか？",multiple_choice,"少量データ学習,大量データ,教師なし,強化学習","少量データ学習","データ取得困難な場面で有効。",0.4,機械学習,学習形態,FewShot
"Zero-shot Learningの特徴はどれか？",multiple_choice,"未学習クラス対応,必ず高精度,教師あり,再学習","未学習クラス対応","事前知識を活用。",0.6,機械学習,学習形態,ZeroShot
"AutoMLの目的はどれか？",multiple_choice,"モデル設計自動化,精度低下,学習不要,推論不可","モデル設計自動化","人手を減らす。",0.4,MLOps,自動化,AutoML
"Neural Architecture Searchが探索するものはどれか？",multiple_choice,"ネットワーク構造,データ,損失関数,評価指標","ネットワーク構造","最適アーキテクチャを自動探索。",0.6,深層学習,NAS,構造
"ハイパーパラメータ最適化でBayesian Optimizationが有効な理由はどれか？",multiple_choice,"少試行で探索,必ず最適,実装容易,学習不要","少試行で探索","評価コストが高い場合に有効。",0.6,機械学習,最適化,BayesianOpt
"Grid Searchの欠点はどれか？",multiple_choice,"計算コストが高い,実装困難,精度低下,最適不可","計算コストが高い","組合せ爆発が起きる。",0.3,機械学習,最適化,GridSearch`;
