export const DeepLearningEngineeringQuestions_CSV = `
question_text,question_type,options,correct_answer,explanation,difficulty,category,subcategory1,subcategory2
"What is the main purpose of a loss function in machine learning?",multiple_choice,"To measure prediction error,To update weights directly,To preprocess data,To reduce model size","To measure prediction error","A loss function quantifies the difference between predicted values and true values, guiding the optimization process.",0.2,MachineLearning,Basics,LossFunction
"What does overfitting mean?",multiple_choice,"Model fits training data too well and generalizes poorly,Model is too simple,Model cannot learn,Training data is insufficient","Model fits training data too well and generalizes poorly","Overfitting occurs when a model captures noise instead of general patterns, reducing performance on unseen data.",0.2,MachineLearning,Basics,Overfitting
"What is the role of regularization?",multiple_choice,"To prevent overfitting,To increase model complexity,To speed up inference,To normalize data","To prevent overfitting","Regularization adds constraints or penalties to reduce model complexity and improve generalization.",0.3,MachineLearning,Regularization,Basics
"Which algorithm is mainly used for classification?",multiple_choice,"Logistic Regression,Linear Regression,K-means,PCA","Logistic Regression","Logistic regression models the probability of class membership and is widely used for classification tasks.",0.2,MachineLearning,Algorithms,Classification
"What does k in k-NN represent?",multiple_choice,"Number of neighbors,Number of clusters,Learning rate,Feature count","Number of neighbors","k determines how many nearby data points are considered when making a prediction.",0.2,MachineLearning,Algorithms,kNN
"What is the main idea of Support Vector Machines?",multiple_choice,"Maximize margin between classes,Minimize data size,Increase depth,Reduce features","Maximize margin between classes","SVM seeks a decision boundary that maximizes the margin between different classes.",0.4,MachineLearning,Algorithms,SVM
"What is a key advantage of decision trees?",multiple_choice,"Interpretability,High accuracy always,Low memory usage,Continuous optimization","Interpretability","Decision trees are easy to understand and visualize, making them highly interpretable.",0.3,MachineLearning,Algorithms,DecisionTree
"What does ensemble learning aim to do?",multiple_choice,"Combine multiple models,Reduce dataset size,Eliminate features,Speed up labeling","Combine multiple models","Ensemble methods improve performance by aggregating predictions from multiple models.",0.3,MachineLearning,Ensemble,Basics
"What is bagging mainly used for?",multiple_choice,"Reducing variance,Reducing bias,Feature selection,Dimensionality reduction","Reducing variance","Bagging stabilizes predictions by training models on different bootstrap samples.",0.4,MachineLearning,Ensemble,Bagging
"What is boosting designed to improve?",multiple_choice,"Bias and variance,Only variance,Only bias,Data quality","Bias and variance","Boosting focuses on hard examples to progressively improve model performance.",0.4,MachineLearning,Ensemble,Boosting
"What does gradient descent optimize?",multiple_choice,"Loss function,Dataset size,Feature count,Model depth","Loss function","Gradient descent iteratively updates parameters to minimize the loss function.",0.2,MachineLearning,Optimization,GradientDescent
"What happens if the learning rate is too small?",multiple_choice,"Slow convergence,Divergence,Overfitting,No gradients","Slow convergence","A very small learning rate makes training extremely slow.",0.2,MachineLearning,Optimization,LearningRate
"What is the purpose of feature scaling?",multiple_choice,"Stabilize optimization,Increase data size,Remove noise,Encode labels","Stabilize optimization","Scaling ensures features contribute equally during optimization.",0.3,Preprocessing,Scaling,Basics
"What is normalization?",multiple_choice,"Scaling values to a fixed range,Removing outliers,Encoding categories,Reducing dimensions","Scaling values to a fixed range","Normalization typically rescales data into a range such as 0 to 1.",0.3,Preprocessing,Scaling,Normalization
"What is standardization?",multiple_choice,"Mean 0 and variance 1,0 to 1 scaling,Log transformation,Discretization","Mean 0 and variance 1","Standardization centers data and scales it by standard deviation.",0.3,Preprocessing,Scaling,Standardization
"What is PCA mainly used for?",multiple_choice,"Dimensionality reduction,Classification,Regression,Clustering","Dimensionality reduction","PCA reduces dimensions while preserving maximum variance.",0.3,MachineLearning,DimensionalityReduction,PCA
"What is the main risk of high-dimensional data?",multiple_choice,"Curse of dimensionality,Over-regularization,Underfitting,Label noise","Curse of dimensionality","High dimensions increase sparsity and degrade model performance.",0.4,MachineLearning,Basics,HighDimensional
"What does a confusion matrix show?",multiple_choice,"Prediction results by class,Loss curve,Feature importance,Model structure","Prediction results by class","It summarizes correct and incorrect predictions for each class.",0.3,MachineLearning,Evaluation,ConfusionMatrix
"What does precision measure?",multiple_choice,"Correct positive predictions ratio,All correct predictions,Error rate,Recall","Correct positive predictions ratio","Precision evaluates how many predicted positives are actually correct.",0.3,MachineLearning,Evaluation,Precision
"What does recall measure?",multiple_choice,"Detected positives ratio,Prediction confidence,Accuracy,Loss","Detected positives ratio","Recall measures how many actual positives were correctly identified.",0.3,MachineLearning,Evaluation,Recall
"When is F1-score useful?",multiple_choice,"Imbalanced datasets,Balanced datasets only,Regression tasks,Clustering","Imbalanced datasets","F1-score balances precision and recall, useful for skewed class distributions.",0.3,MachineLearning,Evaluation,F1
"What does ROC-AUC represent?",multiple_choice,"Overall classification ability,Training speed,Model size,Loss stability","Overall classification ability","ROC-AUC evaluates how well classes are separable across thresholds.",0.4,MachineLearning,Evaluation,AUC
"What is cross-validation used for?",multiple_choice,"Generalization estimation,Speeding training,Feature creation,Label correction","Generalization estimation","Cross-validation provides robust performance estimates.",0.3,MachineLearning,Evaluation,CrossValidation
"What is the purpose of train-test split?",multiple_choice,"Evaluate generalization,Increase accuracy,Reduce features,Normalize data","Evaluate generalization","It separates training from unbiased evaluation data.",0.2,MachineLearning,Basics,TrainTestSplit
"What is a neural network neuron inspired by?",multiple_choice,"Biological neurons,Decision trees,Statistics rules,Databases","Biological neurons","Artificial neurons are inspired by biological neural cells.",0.2,DeepLearning,Basics,Neuron
"What is an activation function?",multiple_choice,"Introduces non-linearity,Updates weights,Calculates loss,Normalizes data","Introduces non-linearity","Activation functions allow neural networks to model complex patterns.",0.2,DeepLearning,Basics,Activation
"Why is ReLU widely used?",multiple_choice,"Reduces vanishing gradients,Always optimal,Probabilistic output,Linear behavior","Reduces vanishing gradients","ReLU mitigates gradient vanishing and is computationally simple.",0.3,DeepLearning,Activation,ReLU
"What problem does sigmoid activation have?",multiple_choice,"Vanishing gradients,No output range,Exploding gradients,Non-differentiable","Vanishing gradients","Sigmoid saturates at extremes, causing gradients to vanish.",0.3,DeepLearning,Activation,Sigmoid
"What is backpropagation?",multiple_choice,"Gradient computation method,Forward inference,Data preprocessing,Model evaluation","Gradient computation method","Backpropagation efficiently computes gradients using the chain rule.",0.3,DeepLearning,Training,Backpropagation
"What is the role of an optimizer?",multiple_choice,"Update parameters,Compute loss,Encode data,Evaluate model","Update parameters","Optimizers adjust weights based on gradients.",0.2,DeepLearning,Optimization,Optimizer
"What does Adam optimizer combine?",multiple_choice,"Momentum and adaptive learning rates,Only SGD,Only RMSProp,Second derivatives","Momentum and adaptive learning rates","Adam combines momentum and RMSProp-style adaptation.",0.4,DeepLearning,Optimization,Adam
"What is batch size?",multiple_choice,"Samples per update,Total dataset size,Epoch count,Feature count","Samples per update","Batch size defines how many samples are processed before updating weights.",0.2,DeepLearning,Training,BatchSize
"What is an epoch?",multiple_choice,"One full pass over data,One batch update,One gradient step,One feature cycle","One full pass over data","An epoch means the model has seen all training data once.",0.2,DeepLearning,Training,Epoch
"What is dropout used for?",multiple_choice,"Prevent overfitting,Speed inference,Normalize outputs,Increase depth","Prevent overfitting","Dropout randomly deactivates neurons during training.",0.3,DeepLearning,Regularization,Dropout
"What is batch normalization?",multiple_choice,"Normalize layer inputs,Reduce dataset size,Encode labels,Clip gradients","Normalize layer inputs","BatchNorm stabilizes and accelerates training.",0.3,DeepLearning,Normalization,BatchNorm
"What is CNN mainly used for?",multiple_choice,"Image processing,Text translation,Time series only,Tabular data only","Image processing","CNNs are specialized for spatial data like images.",0.2,DeepLearning,CNN,Basics
"What is convolution in CNN?",multiple_choice,"Local feature extraction,Global averaging,Random sampling,Dimensional increase","Local feature extraction","Convolution extracts local patterns using filters.",0.2,DeepLearning,CNN,Convolution
"What is pooling used for?",multiple_choice,"Reduce spatial size,Increase resolution,Add noise,Normalize channels","Reduce spatial size","Pooling downsamples feature maps and reduces computation.",0.2,DeepLearning,CNN,Pooling
"What is the main advantage of CNN weight sharing?",multiple_choice,"Parameter reduction,Higher resolution,Better labels,No training needed","Parameter reduction","Shared weights reduce parameters and improve generalization.",0.3,DeepLearning,CNN,WeightSharing
"What is RNN suitable for?",multiple_choice,"Sequential data,Images only,Static data only,Clustering","Sequential data","RNNs handle temporal dependencies.",0.3,DeepLearning,RNN,Basics
"What problem do basic RNNs suffer from?",multiple_choice,"Vanishing gradients,Overfitting only,Non-linearity,High accuracy","Vanishing gradients","Gradients decay over long sequences.",0.3,DeepLearning,RNN,VanishingGradient
"What is the purpose of LSTM gates?",multiple_choice,"Control information flow,Increase depth,Reduce features,Speed inference","Control information flow","Gates regulate memory retention and forgetting.",0.3,DeepLearning,RNN,LSTM
"What is GRU compared to LSTM?",multiple_choice,"Simpler architecture,More complex,Non-recurrent,Untrainable","Simpler architecture","GRU has fewer parameters than LSTM.",0.3,DeepLearning,RNN,GRU
"What does attention mechanism do?",multiple_choice,"Focus on relevant parts,Reduce data size,Normalize weights,Encode labels","Focus on relevant parts","Attention dynamically weights important inputs.",0.3,DeepLearning,Attention,Basics
"What is Transformer based on?",multiple_choice,"Self-attention,Convolution,RNN recurrence,Decision trees","Self-attention","Transformers rely entirely on attention mechanisms.",0.3,DeepLearning,Transformer,Basics
"Why does a neural network need non-linear activation functions?",multiple_choice,"To model complex patterns,To reduce data size,To normalize inputs,To speed up inference","To model complex patterns","Without non-linearity, stacked layers collapse into a linear model and cannot represent complex functions.",0.4,DeepLearning,Theory,NonLinearity
"What happens if all activation functions are linear?",multiple_choice,"The network becomes equivalent to a linear model,Training stops,Gradients explode,Accuracy always improves","The network becomes equivalent to a linear model","Multiple linear transformations compose into a single linear transformation.",0.4,DeepLearning,Theory,LinearModel
"What is the vanishing gradient problem?",multiple_choice,"Gradients become very small during backpropagation,Gradients become infinite,Loss becomes zero,Weights stop updating immediately","Gradients become very small during backpropagation","Small gradients prevent effective learning in deep networks.",0.4,DeepLearning,Training,VanishingGradient
"What is the exploding gradient problem?",multiple_choice,"Gradients grow excessively large,Gradients disappear,Loss is minimized,Model converges faster","Gradients grow excessively large","Large gradients cause unstable updates and numerical issues.",0.4,DeepLearning,Training,ExplodingGradient
"Why is gradient clipping used?",multiple_choice,"To prevent exploding gradients,To speed up convergence,To remove noise,To normalize features","To prevent exploding gradients","Clipping limits gradient magnitude for stable training.",0.4,DeepLearning,Optimization,GradientClipping
"What is the main role of weight initialization?",multiple_choice,"Stabilize training,Guarantee accuracy,Reduce dataset size,Eliminate bias","Stabilize training","Proper initialization avoids vanishing or exploding gradients.",0.4,DeepLearning,Initialization,Basics
"For which activation is Xavier initialization most suitable?",multiple_choice,"tanh,ReLU,Sigmoid only,Softmax","tanh","Xavier initialization maintains variance for symmetric activations.",0.5,DeepLearning,Initialization,Xavier
"For which activation is He initialization designed?",multiple_choice,"ReLU,tanh,Sigmoid,Softmax","ReLU","He initialization accounts for ReLUâ€™s zeroing behavior.",0.5,DeepLearning,Initialization,He
"What does batch normalization reduce?",multiple_choice,"Internal covariate shift,Dataset size,Model depth,Feature count","Internal covariate shift","It stabilizes input distributions of layers during training.",0.4,DeepLearning,Normalization,BatchNorm
"Why can batch normalization act as regularization?",multiple_choice,"Introduces noise during training,Reduces parameters,Stops learning,Enforces sparsity","Introduces noise during training","Mini-batch statistics add stochasticity similar to regularization.",0.5,DeepLearning,Normalization,Regularization
"What is layer normalization best suited for?",multiple_choice,"Sequence models,Large batch training only,Image classification,Clustering","Sequence models","LayerNorm is independent of batch size, suitable for NLP.",0.5,DeepLearning,Normalization,LayerNorm
"What is the key difference between BatchNorm and LayerNorm?",multiple_choice,"Normalization axis,Batch size requirement,Training speed only,Loss function","Normalization axis","They normalize across different dimensions.",0.5,DeepLearning,Normalization,Comparison
"What is the purpose of dropout?",multiple_choice,"Reduce overfitting,Increase training speed,Normalize gradients,Expand network","Reduce overfitting","Dropout prevents co-adaptation of neurons.",0.4,DeepLearning,Regularization,Dropout
"What happens if dropout rate is too high?",multiple_choice,"Underfitting,Better generalization always,Training divergence,No effect","Underfitting","Too much information is removed during training.",0.4,DeepLearning,Regularization,DropoutRate
"What does an epoch represent?",multiple_choice,"One full pass through the dataset,One batch update,One gradient step,One feature update","One full pass through the dataset","Epochs count how many times the model sees all data.",0.3,DeepLearning,Training,Epoch
"What is mini-batch training?",multiple_choice,"Training on small subsets of data,Training on full dataset only,Online learning only,Inference method","Training on small subsets of data","It balances stability and efficiency.",0.3,DeepLearning,Training,MiniBatch
"What is the advantage of mini-batch training?",multiple_choice,"Efficient and stable updates,Guaranteed convergence,No memory usage,Exact gradients","Efficient and stable updates","Mini-batches reduce variance and improve hardware utilization.",0.3,DeepLearning,Training,MiniBatch
"What does learning rate control?",multiple_choice,"Step size of updates,Number of layers,Model size,Loss shape","Step size of updates","Learning rate determines how much parameters change per update.",0.3,DeepLearning,Optimization,LearningRate
"What happens if the learning rate is too large?",multiple_choice,"Training diverges,Faster convergence always,No updates occur,Perfect accuracy","Training diverges","Updates overshoot the optimum.",0.3,DeepLearning,Optimization,LearningRate
"What is a learning rate scheduler?",multiple_choice,"Adjusts learning rate during training,Normalizes gradients,Selects features,Stops training","Adjusts learning rate during training","Schedulers improve convergence behavior.",0.4,DeepLearning,Optimization,Scheduler
"What is cosine annealing?",multiple_choice,"Periodic learning rate decay,Linear decay,Random decay,Constant rate","Periodic learning rate decay","It follows a cosine-shaped schedule.",0.5,DeepLearning,Optimization,CosineAnnealing
"What is early stopping?",multiple_choice,"Stopping training when validation worsens,Reducing batch size,Freezing layers,Increasing learning rate","Stopping training when validation worsens","It prevents overfitting by monitoring validation loss.",0.3,DeepLearning,Regularization,EarlyStopping
"What is transfer learning?",multiple_choice,"Reusing pre-trained models,Training from scratch only,Unsupervised learning,Model compression","Reusing pre-trained models","Knowledge from large datasets is transferred to new tasks.",0.3,DeepLearning,TransferLearning,Basics
"Why freeze lower layers during fine-tuning?",multiple_choice,"Preserve learned features,Speed inference,Reduce dataset size,Increase randomness","Preserve learned features","Lower layers capture general patterns.",0.4,DeepLearning,TransferLearning,FineTuning
"What is catastrophic forgetting?",multiple_choice,"Loss of old knowledge when learning new tasks,Overfitting,Vanishing gradients,Model collapse","Loss of old knowledge when learning new tasks","Common in continual learning.",0.5,DeepLearning,ContinualLearning,Forgetting
"What is multi-task learning?",multiple_choice,"Training on multiple related tasks,Training with many datasets only,Parallel inference,Ensemble learning","Training on multiple related tasks","Shared representations improve generalization.",0.4,DeepLearning,LearningStrategy,MultiTask
"What is knowledge distillation?",multiple_choice,"Transferring knowledge to smaller models,Increasing model size,Removing labels,Data augmentation","Transferring knowledge to smaller models","Teacher models guide student models.",0.5,DeepLearning,Compression,Distillation
"Why are soft targets used in distillation?",multiple_choice,"Convey class similarity,Reduce computation,Increase noise,Guarantee accuracy","Convey class similarity","Soft probabilities carry richer information.",0.5,DeepLearning,Compression,SoftTargets
"What is the main goal of model compression?",multiple_choice,"Reduce size and latency,Increase depth,Improve labels,Eliminate training","Reduce size and latency","Important for deployment on limited hardware.",0.4,DeepLearning,Compression,Goal
"What is pruning?",multiple_choice,"Removing unimportant weights,Adding layers,Normalizing gradients,Augmenting data","Removing unimportant weights","Pruning reduces model complexity.",0.4,DeepLearning,Compression,Pruning
"What is quantization?",multiple_choice,"Reducing numerical precision,Increasing precision,Feature scaling,Normalization","Reducing numerical precision","It reduces memory and computation cost.",0.4,DeepLearning,Compression,Quantization
"What is the main use of CNNs?",multiple_choice,"Image-related tasks,Text generation,Tabular regression,Clustering","Image-related tasks","CNNs exploit spatial locality.",0.3,DeepLearning,CNN,Usage
"What does stride control in convolution?",multiple_choice,"Filter movement step,Number of channels,Kernel size,Padding size","Filter movement step","Stride affects output spatial resolution.",0.3,DeepLearning,CNN,Stride
"What is padding used for in CNNs?",multiple_choice,"Control output size,Increase noise,Reduce channels,Speed training","Control output size","Padding preserves spatial dimensions.",0.3,DeepLearning,CNN,Padding
"What is dilated convolution used for?",multiple_choice,"Expand receptive field efficiently,Reduce parameters always,Normalize features,Prevent overfitting","Expand receptive field efficiently","It captures wider context with fewer parameters.",0.5,DeepLearning,CNN,DilatedConv
"What is global average pooling?",multiple_choice,"Averaging spatial dimensions,Replacing convolution,Data normalization,Feature scaling","Averaging spatial dimensions","It reduces parameters and overfitting.",0.4,DeepLearning,CNN,GAP
"What is the benefit of residual connections?",multiple_choice,"Easier gradient flow,Increased parameters only,Slower training,Shallower networks","Easier gradient flow","Residuals help train very deep networks.",0.4,DeepLearning,CNN,Residual
"What problem do residual networks address?",multiple_choice,"Degradation problem,Overfitting only,Data imbalance,Label noise","Degradation problem","Accuracy degrades as depth increases without residuals.",0.4,DeepLearning,CNN,ResNet
"What is an RNN mainly designed for?",multiple_choice,"Sequential data,Images only,Static vectors,Clustering","Sequential data","RNNs process temporal dependencies.",0.3,DeepLearning,RNN,Usage
"What is the limitation of basic RNNs?",multiple_choice,"Long-term dependency issues,High accuracy,Parallel computation,Low memory","Long-term dependency issues","Gradients vanish over long sequences.",0.4,DeepLearning,RNN,Limitation
"What does LSTM introduce to solve RNN limitations?",multiple_choice,"Gating mechanisms,More layers,Convolution,Pooling","Gating mechanisms","Gates control memory flow.",0.4,DeepLearning,RNN,LSTM
"What is a key advantage of GRU?",multiple_choice,"Simpler than LSTM,Always better accuracy,No recurrence,Stateless","Simpler than LSTM","GRU uses fewer parameters.",0.4,DeepLearning,RNN,GRU
"What does attention allow a model to do?",multiple_choice,"Focus on important inputs,Ignore gradients,Reduce batch size,Normalize loss","Focus on important inputs","Attention dynamically weights information.",0.3,DeepLearning,Attention,Function
"What is self-attention?",multiple_choice,"Attention within the same sequence,Cross-modal attention,External memory,Pooling","Attention within the same sequence","Each element attends to others in the sequence.",0.4,DeepLearning,Attention,SelfAttention
"What is the core component of Transformers?",multiple_choice,"Self-attention mechanism,CNN layers,RNN recurrence,Decision trees","Self-attention mechanism","Transformers remove recurrence entirely.",0.3,DeepLearning,Transformer,Core
"What is positional encoding used for in Transformers?",multiple_choice,"Inject sequence order information,Reduce computation,Normalize embeddings,Clip gradients","Inject sequence order information","Transformers lack inherent order, so positional encodings provide token position information.",0.4,DeepLearning,Transformer,PositionEncoding
"What is the purpose of attention masks?",multiple_choice,"Prevent attending to invalid positions,Increase learning rate,Normalize gradients,Compress data","Prevent attending to invalid positions","Masks block padding tokens or future tokens from attention.",0.4,DeepLearning,Transformer,AttentionMask
"What is a causal mask mainly used for?",multiple_choice,"Prevent future information leakage,Speed up training,Normalize loss,Reduce parameters","Prevent future information leakage","Causal masks ensure autoregressive generation.",0.4,DeepLearning,Transformer,CausalMask
"Why does Transformer scale poorly with long sequences?",multiple_choice,"Self-attention has quadratic complexity,Too many parameters,Activation saturation,Gradient clipping","Self-attention has quadratic complexity","Attention cost grows with sequence length squared.",0.5,DeepLearning,Transformer,Complexity
"What is the key idea behind BERT?",multiple_choice,"Bidirectional context learning,Autoregressive generation,Image understanding,Reinforcement learning","Bidirectional context learning","BERT considers both left and right context simultaneously.",0.4,NLP,BERT,Concept
"What pretraining task is used in BERT?",multiple_choice,"Masked Language Modeling,Next word prediction,Image captioning,Clustering","Masked Language Modeling","Random tokens are masked and predicted.",0.4,NLP,BERT,Pretraining
"What is the goal of Next Sentence Prediction?",multiple_choice,"Learn sentence relationships,Improve tokenization,Reduce parameters,Speed inference","Learn sentence relationships","NSP helps model inter-sentence coherence.",0.4,NLP,BERT,NSP
"Why is BERT not ideal for text generation?",multiple_choice,"Not autoregressive,Too small,No embeddings,No attention","Not autoregressive","BERT sees both directions and cannot generate sequentially.",0.4,NLP,BERT,Limitation
"What is GPT mainly designed for?",multiple_choice,"Autoregressive text generation,Bidirectional understanding,Image segmentation,Speech recognition","Autoregressive text generation","GPT predicts the next token based on previous tokens.",0.4,NLP,GPT,Usage
"Why does GPT use only a decoder stack?",multiple_choice,"Supports autoregressive modeling,Reduces parameters,Encoder unnecessary,Faster training always","Supports autoregressive modeling","Decoder-only architecture enforces causal structure.",0.4,NLP,GPT,Architecture
"What is teacher forcing?",multiple_choice,"Using ground-truth tokens during training,Freezing layers,Reducing batch size,Clipping gradients","Using ground-truth tokens during training","It stabilizes sequence-to-sequence training.",0.5,NLP,Seq2Seq,TeacherForcing
"What problem can teacher forcing cause?",multiple_choice,"Exposure bias,Overfitting only,Vanishing gradients,Mode collapse","Exposure bias","Training and inference conditions differ.",0.5,NLP,Seq2Seq,ExposureBias
"What is beam search used for?",multiple_choice,"Improve sequence generation quality,Reduce training time,Normalize loss,Augment data","Improve sequence generation quality","It explores multiple candidate sequences.",0.4,NLP,Decoding,BeamSearch
"What is a drawback of greedy decoding?",multiple_choice,"Prone to local optima,High computation cost,Hard to implement,Requires labels","Prone to local optima","Choosing locally best tokens may harm global quality.",0.4,NLP,Decoding,Greedy
"What does BLEU score measure?",multiple_choice,"n-gram overlap with reference,Semantic similarity,Grammar correctness,Perplexity","n-gram overlap with reference","BLEU evaluates machine translation output.",0.4,NLP,Evaluation,BLEU
"What task is ROUGE mainly used for?",multiple_choice,"Text summarization,Translation,Classification,Clustering","Text summarization","ROUGE measures overlap with reference summaries.",0.4,NLP,Evaluation,ROUGE
"What does low perplexity indicate?",multiple_choice,"Good language modeling,Overfitting,Short sequences,High bias","Good language modeling","Lower uncertainty in predicting next tokens.",0.4,NLP,Evaluation,Perplexity
"What is the role of word embeddings?",multiple_choice,"Map tokens to continuous vectors,Classify text,Compute loss,Reduce length","Map tokens to continuous vectors","Embeddings capture semantic similarity.",0.3,NLP,Embedding,Basics
"What happens if embedding dimension is too large?",multiple_choice,"Risk of overfitting,Always better accuracy,No training,Vanishing gradients","Risk of overfitting","Excessive parameters reduce generalization.",0.4,NLP,Embedding,Dimension
"What is negative sampling used for?",multiple_choice,"Reduce softmax computation cost,Increase accuracy always,Normalize data,Encode labels","Reduce softmax computation cost","It approximates full softmax efficiently.",0.5,NLP,Word2Vec,NegativeSampling
"What is a key feature of ELMo?",multiple_choice,"Context-dependent embeddings,Fixed vectors,Autoregressive only,No attention","Context-dependent embeddings","Same word has different representations by context.",0.5,NLP,Embedding,ELMo
"What distinguishes contextual embeddings from static ones?",multiple_choice,"Depend on surrounding words,Lower dimension,No training,Task-specific only","Depend on surrounding words","Contextual meaning is captured.",0.4,NLP,Embedding,Contextual
"What is fine-tuning in NLP models?",multiple_choice,"Adapting pre-trained models to tasks,Training from scratch,Feature selection,Tokenization","Adapting pre-trained models to tasks","Weights are updated for downstream tasks.",0.3,NLP,TransferLearning,FineTuning
"What is the risk of full fine-tuning?",multiple_choice,"Catastrophic forgetting,No convergence,Zero gradients,Underfitting","Catastrophic forgetting","Pretrained knowledge may be overwritten.",0.5,NLP,TransferLearning,Risk
"What is prompt learning?",multiple_choice,"Task adaptation via input design,Weight retraining,Model compression,Data augmentation","Task adaptation via input design","Model parameters remain fixed.",0.5,NLP,PromptLearning,Concept
"What is few-shot learning?",multiple_choice,"Learning from few examples,Learning without labels,Reinforcement learning only,Unsupervised clustering","Learning from few examples","Effective when labeled data is scarce.",0.4,MachineLearning,LearningParadigm,FewShot
"What is zero-shot learning?",multiple_choice,"Handling unseen classes,Training with zero data,Only unsupervised learning,Overfitting","Handling unseen classes","Relies on prior knowledge.",0.5,MachineLearning,LearningParadigm,ZeroShot
"What is AutoML?",multiple_choice,"Automating model design and tuning,Manual optimization,Inference system,Data labeling","Automating model design and tuning","Reduces human effort in ML pipelines.",0.4,MLOps,Automation,AutoML
"What does Neural Architecture Search optimize?",multiple_choice,"Network structures,Datasets,Loss functions,Labels","Network structures","NAS explores architectures automatically.",0.5,DeepLearning,NAS,Architecture
"Why is Bayesian optimization effective?",multiple_choice,"Efficient with few evaluations,Always finds optimum,Simple implementation,No assumptions","Efficient with few evaluations","Useful when evaluations are expensive.",0.5,MachineLearning,Optimization,BayesianOptimization
"What is a drawback of grid search?",multiple_choice,"Computationally expensive,Low accuracy,Hard to code,No reproducibility","Computationally expensive","Search space grows exponentially.",0.3,MachineLearning,Optimization,GridSearch
"What is random search good at?",multiple_choice,"Exploring high-dimensional spaces,Guaranteeing optimality,Reducing bias,Eliminating tuning","Exploring high-dimensional spaces","Often more efficient than grid search.",0.4,MachineLearning,Optimization,RandomSearch
"What is the purpose of hyperparameter tuning?",multiple_choice,"Improve model performance,Reduce dataset size,Fix labels,Normalize features","Improve model performance","Hyperparameters control learning behavior.",0.3,MachineLearning,Optimization,Hyperparameters
"What is cross-validation mainly used for?",multiple_choice,"Estimate generalization,Speed training,Feature creation,Label correction","Estimate generalization","It reduces evaluation bias.",0.3,MachineLearning,Evaluation,CrossValidation
"What does k-fold cross-validation do?",multiple_choice,"Split data into k subsets,Increase features,Normalize gradients,Remove noise","Split data into k subsets","Each subset is used for validation once.",0.3,MachineLearning,Evaluation,KFold
"What happens if k is very large?",multiple_choice,"High computational cost,Low accuracy,Underfitting,Data leakage","High computational cost","Training is repeated many times.",0.4,MachineLearning,Evaluation,KFold
"What does a confusion matrix summarize?",multiple_choice,"Classification outcomes,Loss trends,Feature weights,Model structure","Classification outcomes","It shows TP, FP, TN, FN.",0.3,MachineLearning,Evaluation,ConfusionMatrix
"What does false negative mean?",multiple_choice,"Positive predicted as negative,Negative predicted as positive,Correct positive,Correct negative","Positive predicted as negative","It represents missed detections.",0.3,MachineLearning,Evaluation,FalseNegative
"What metric should be emphasized when missing positives is costly?",multiple_choice,"Recall,Precision,Accuracy,F1","Recall","High recall reduces false negatives.",0.3,MachineLearning,Evaluation,Recall
"What metric should be emphasized when false alarms are costly?",multiple_choice,"Precision,Recall,Accuracy,ROC","Precision","High precision reduces false positives.",0.3,MachineLearning,Evaluation,Precision
"What is the benefit of F1-score?",multiple_choice,"Balances precision and recall,Maximizes accuracy,Works for regression,Simplifies loss","Balances precision and recall","Useful for imbalanced data.",0.3,MachineLearning,Evaluation,F1
"What does ROC curve plot?",multiple_choice,"TPR vs FPR,Loss vs epoch,Precision vs recall,Accuracy vs time","TPR vs FPR","It visualizes threshold behavior.",0.4,MachineLearning,Evaluation,ROC
"What does AUC represent?",multiple_choice,"Overall discrimination ability,Training speed,Model complexity,Loss stability","Overall discrimination ability","Higher AUC means better class separation.",0.3,MachineLearning,Evaluation,AUC
"Why can neural networks solve non-linearly separable problems?",multiple_choice,"They use non-linear activation functions,They have many layers only,They use large datasets,They have many parameters","They use non-linear activation functions","Non-linear activations allow networks to model complex decision boundaries.",0.4,DeepLearning,Theory,NonLinearity
"What happens when stride is increased in a CNN?",multiple_choice,"Output feature map becomes smaller,Accuracy always improves,Computation increases,More features are extracted","Output feature map becomes smaller","Larger strides reduce spatial resolution.",0.3,DeepLearning,CNN,Stride
"What is the main purpose of zero padding?",multiple_choice,"Preserve output size,Reduce noise,Initialize weights,Normalize data","Preserve output size","Padding controls spatial dimensions after convolution.",0.3,DeepLearning,CNN,Padding
"What is the benefit of dilated convolution?",multiple_choice,"Expand receptive field with fewer parameters,Guarantee higher accuracy,Prevent vanishing gradients,Increase depth","Expand receptive field with fewer parameters","Dilated convolutions capture wider context efficiently.",0.5,DeepLearning,CNN,DilatedConv
"Why are fully connected layers used after CNN layers?",multiple_choice,"Integrate extracted features for prediction,Reduce image size,Add noise,Increase resolution","Integrate extracted features for prediction","They map learned features to final outputs.",0.3,DeepLearning,CNN,FullyConnected
"What is an advantage of global average pooling?",multiple_choice,"Reduces parameters and overfitting,Always improves accuracy,Enables sequence modeling,Replaces attention","Reduces parameters and overfitting","GAP removes the need for large fully connected layers.",0.4,DeepLearning,CNN,GAP
"What is the role of attention masks?",multiple_choice,"Disable attention to invalid positions,Control learning rate,Block gradients,Remove features","Disable attention to invalid positions","Masks prevent attending to padding or future tokens.",0.4,DeepLearning,Transformer,Mask
"What tasks are encoder-decoder architectures mainly used for?",multiple_choice,"Sequence-to-sequence tasks,Image classification,Clustering,Anomaly detection","Sequence-to-sequence tasks","Common in translation and summarization.",0.4,NLP,EncoderDecoder,Seq2Seq
"Why is teacher forcing used in seq2seq training?",multiple_choice,"Stabilize training,Speed up inference,Reduce model size,Eliminate loss","Stabilize training","It reduces error accumulation during training.",0.5,NLP,Seq2Seq,TeacherForcing
"What causes exposure bias?",multiple_choice,"Mismatch between training and inference inputs,Small datasets,Shallow models,High learning rate","Mismatch between training and inference inputs","Training uses ground truth, inference uses predictions.",0.6,NLP,Seq2Seq,ExposureBias
"What is the goal of beam search?",multiple_choice,"Improve generated sequence quality,Reduce training time,Compress data,Regularize models","Improve generated sequence quality","It keeps multiple hypotheses during decoding.",0.4,NLP,Decoding,BeamSearch
"What is a weakness of greedy decoding?",multiple_choice,"Prone to local optima,High computation cost,Complex implementation,Requires labels","Prone to local optima","It selects the best local choice at each step.",0.4,NLP,Decoding,Greedy
"What does BLEU score evaluate?",multiple_choice,"n-gram overlap with reference,Semantic similarity only,Grammar correctness,Loss value","n-gram overlap with reference","Commonly used for machine translation.",0.4,NLP,Evaluation,BLEU
"What task is ROUGE primarily used for?",multiple_choice,"Text summarization,Image classification,Regression,Clustering","Text summarization","ROUGE compares generated and reference summaries.",0.4,NLP,Evaluation,ROUGE
"What does low perplexity indicate in language models?",multiple_choice,"Good predictive performance,Overfitting,Short sequences,High bias","Good predictive performance","Lower uncertainty in next-token prediction.",0.4,NLP,Evaluation,Perplexity
"What is the function of embedding layers?",multiple_choice,"Convert discrete tokens to vectors,Classify data,Compute gradients,Reduce sequence length","Convert discrete tokens to vectors","They capture semantic relationships.",0.3,NLP,Embedding,Basics
"What is a risk of very large embedding dimensions?",multiple_choice,"Overfitting,Guaranteed accuracy,No convergence,Vanishing gradients","Overfitting","Too many parameters hurt generalization.",0.4,NLP,Embedding,Dimension
"What is negative sampling used for?",multiple_choice,"Reduce computational cost of softmax,Increase accuracy always,Normalize inputs,Encode outputs","Reduce computational cost of softmax","Efficient approximation in word embeddings.",0.5,NLP,Word2Vec,NegativeSampling
"What is a characteristic of ELMo embeddings?",multiple_choice,"Context-dependent representations,Fixed vectors,Autoregressive only,No attention","Context-dependent representations","Word meaning depends on context.",0.5,NLP,Embedding,ELMo
"Why are GPT models decoder-only?",multiple_choice,"They support autoregressive generation,They reduce parameters,Encoders are useless,They improve classification","They support autoregressive generation","Decoder-only enforces causal attention.",0.5,NLP,GPT,Architecture
"What is the purpose of causal masking?",multiple_choice,"Prevent access to future tokens,Normalize attention scores,Reduce computation,Increase randomness","Prevent access to future tokens","Ensures autoregressive behavior.",0.4,NLP,Transformer,CausalMask
"What is a risk of updating all layers during fine-tuning?",multiple_choice,"Catastrophic forgetting,Guaranteed improvement,No convergence,Inference failure","Catastrophic forgetting","Pretrained knowledge may be overwritten.",0.5,DeepLearning,TransferLearning,Risk
"What is catastrophic forgetting?",multiple_choice,"Losing old knowledge when learning new tasks,Overfitting,Exploding gradients,Model collapse","Losing old knowledge when learning new tasks","Occurs in continual learning.",0.5,DeepLearning,ContinualLearning,Forgetting
"What is an advantage of multi-task learning?",multiple_choice,"Shared representations improve generalization,Always higher accuracy,No training needed,Single dataset","Shared representations improve generalization","Related tasks reinforce learning.",0.4,DeepLearning,LearningStrategy,MultiTask
"What is knowledge distillation used for?",multiple_choice,"Transfer knowledge to smaller models,Increase model size,Remove labels,Normalize gradients","Transfer knowledge to smaller models","Teacher-student training paradigm.",0.5,DeepLearning,Compression,Distillation
"Why use soft targets in distillation?",multiple_choice,"Convey inter-class relationships,Reduce noise,Speed inference,Guarantee accuracy","Convey inter-class relationships","Soft probabilities carry richer information.",0.5,DeepLearning,Compression,SoftTargets
"What is the goal of federated learning?",multiple_choice,"Train without sharing raw data,Increase accuracy always,Reduce model size,Centralize data","Train without sharing raw data","It preserves data privacy.",0.5,MachineLearning,DistributedLearning,Federated
"What does differential privacy aim to guarantee?",multiple_choice,"Limit impact of individual data,Complete anonymity,Perfect accuracy,No training","Limit impact of individual data","Protects individual contribution.",0.6,AI_Ethics,Privacy,DifferentialPrivacy
"What does LIME provide?",multiple_choice,"Local model explanations,Global architecture,Loss curves,Feature scaling","Local model explanations","Explains individual predictions.",0.5,Interpretability,LIME,Explanation
"What theory underlies SHAP values?",multiple_choice,"Cooperative game theory,Probability theory,Information theory,Linear algebra","Cooperative game theory","SHAP fairly attributes feature contributions.",0.6,Interpretability,SHAP,Theory
"Why is explainable AI important?",multiple_choice,"Trust and accountability,Higher accuracy only,Faster training,Smaller models","Trust and accountability","Essential for real-world deployment.",0.4,AI_Ethics,Explainability,XAI
"What causes bias in AI models?",multiple_choice,"Biased training data,Activation functions,Learning rate,Hardware","Biased training data","Data distribution drives model behavior.",0.4,AI_Ethics,Bias,Cause
"What does demographic parity measure?",multiple_choice,"Equal prediction rates across groups,Overall accuracy,Error minimization,Sequence similarity","Equal prediction rates across groups","A fairness metric.",0.6,AI_Ethics,Fairness,DemographicParity
"What is the purpose of model monitoring?",multiple_choice,"Detect performance degradation,Stop training,Reduce features,Normalize data","Detect performance degradation","Monitoring catches drift in production.",0.4,MLOps,Monitoring,Purpose
"What is data drift?",multiple_choice,"Change in input data distribution,Gradient vanishing,Model collapse,Inference speedup","Change in input data distribution","Input characteristics shift over time.",0.4,MLOps,Drift,DataDrift
"What is concept drift?",multiple_choice,"Change in relationship between inputs and labels,Missing values,Feature scaling,Noise reduction","Change in relationship between inputs and labels","Problem definition evolves.",0.5,MLOps,Drift,ConceptDrift
"When is model retraining required?",multiple_choice,"When performance degrades,When accuracy is high,When data is fixed,During inference only","When performance degrades","Retraining maintains quality.",0.4,MLOps,Operations,Retraining
"What is the benefit of online learning?",multiple_choice,"Adapt to streaming data,Always higher accuracy,No overfitting,No updates","Adapt to streaming data","Models update continuously.",0.4,MachineLearning,LearningParadigm,OnlineLearning
"What problem does vanishing gradient cause?",multiple_choice,"Early layers learn very slowly,Model diverges,Loss oscillates,Overfitting increases","Early layers learn very slowly","Gradients shrink as they propagate backward.",0.4,DeepLearning,Optimization,VanishingGradient
"Which activation function helps mitigate vanishing gradients?",multiple_choice,"ReLU,Sigmoid,Tanh,Step","ReLU","ReLU maintains larger gradients for positive inputs.",0.3,DeepLearning,Activation,ReLU
"What problem is associated with ReLU?",multiple_choice,"Dead neurons,Exploding gradients,Overfitting,Slow convergence","Dead neurons","Neurons may output zero permanently.",0.4,DeepLearning,Activation,ReLU
"What is the advantage of Leaky ReLU?",multiple_choice,"Allows small gradient when inactive,Guarantees sparsity,Always faster,Eliminates overfitting","Allows small gradient when inactive","It reduces dead neuron risk.",0.4,DeepLearning,Activation,LeakyReLU
"What is batch normalization mainly used for?",multiple_choice,"Stabilize and speed up training,Increase model size,Prevent overfitting only,Reduce data","Stabilize and speed up training","It normalizes layer inputs.",0.3,DeepLearning,Normalization,BatchNorm
"Where is batch normalization typically applied?",multiple_choice,"Before or after activation,Only input layer,Only output layer,Only convolution","Before or after activation","Placement depends on architecture.",0.4,DeepLearning,Normalization,BatchNorm
"What is a drawback of batch normalization?",multiple_choice,"Depends on batch size,Always slows inference,No regularization effect,Hard to implement","Depends on batch size","Small batches reduce effectiveness.",0.4,DeepLearning,Normalization,BatchNorm
"What alternative works better with small batch sizes?",multiple_choice,"Layer normalization,Dropout,Batch gradient descent,Data augmentation","Layer normalization","It normalizes per sample.",0.4,DeepLearning,Normalization,LayerNorm
"Why is layer normalization popular in Transformers?",multiple_choice,"Independent of batch size,Improves CNNs,Reduces parameters,Adds recurrence","Independent of batch size","Essential for stable attention training.",0.3,DeepLearning,Transformer,LayerNorm
"What is dropout used for?",multiple_choice,"Prevent overfitting,Increase parameters,Speed inference,Normalize data","Prevent overfitting","Randomly deactivates neurons.",0.3,DeepLearning,Regularization,Dropout
"What happens if dropout rate is too high?",multiple_choice,"Underfitting,Overfitting,Faster convergence,No effect","Underfitting","Too much information is removed.",0.3,DeepLearning,Regularization,Dropout
"What is early stopping?",multiple_choice,"Stop training when validation loss worsens,Train fewer epochs always,Freeze layers,Reduce batch size","Stop training when validation loss worsens","Prevents overfitting.",0.3,MachineLearning,Training,EarlyStopping
"What is L2 regularization also known as?",multiple_choice,"Weight decay,Lasso,Elastic net,Dropconnect","Weight decay","It penalizes large weights.",0.3,MachineLearning,Regularization,L2
"What effect does L1 regularization have?",multiple_choice,"Encourages sparsity,Prevents vanishing gradients,Normalizes features,Speeds inference","Encourages sparsity","Many weights become zero.",0.4,MachineLearning,Regularization,L1
"What is elastic net?",multiple_choice,"Combination of L1 and L2,Only L1,Only L2,Dropout variant","Combination of L1 and L2","Balances sparsity and stability.",0.4,MachineLearning,Regularization,ElasticNet
"What is data augmentation?",multiple_choice,"Artificially increasing training data,Reducing dataset size,Normalizing labels,Feature selection","Artificially increasing training data","Improves generalization.",0.3,MachineLearning,Data,Augmentation
"Why is augmentation effective?",multiple_choice,"Introduces invariances,Always increases accuracy,Reduces training time,Eliminates bias","Introduces invariances","Models learn robust features.",0.4,MachineLearning,Data,Augmentation
"What is transfer learning?",multiple_choice,"Reuse pretrained models,Training from scratch,Feature engineering only,Model compression","Reuse pretrained models","Leverages prior knowledge.",0.3,DeepLearning,TransferLearning,Concept
"When is freezing layers useful?",multiple_choice,"Small dataset,Large dataset,Random initialization,Online learning","Small dataset","Prevents overfitting.",0.3,DeepLearning,TransferLearning,Freezing
"What is fine-tuning?",multiple_choice,"Updating pretrained weights for new task,Freezing all layers,Training from scratch,Hyperparameter tuning","Updating pretrained weights for new task","Adapts representations.",0.3,DeepLearning,TransferLearning,FineTuning
"What is catastrophic forgetting?",multiple_choice,"Loss of previous knowledge,Exploding gradients,Overfitting,Slow training","Loss of previous knowledge","Occurs in continual learning.",0.4,DeepLearning,ContinualLearning,Forgetting
"What is continual learning?",multiple_choice,"Learning tasks sequentially,Training once only,Batch learning,Unsupervised learning","Learning tasks sequentially","Models adapt over time.",0.4,DeepLearning,ContinualLearning,Concept
"What is multi-head attention?",multiple_choice,"Parallel attention subspaces,Single attention only,Dropout technique,Normalization","Parallel attention subspaces","Captures diverse relations.",0.3,DeepLearning,Transformer,MultiHead
"Why use multiple attention heads?",multiple_choice,"Capture different relationships,Reduce parameters,Speed training,Avoid masking","Capture different relationships","Each head focuses differently.",0.3,DeepLearning,Transformer,MultiHead
"What is the role of the feed-forward network in Transformers?",multiple_choice,"Apply position-wise transformations,Encode order,Normalize attention,Reduce length","Apply position-wise transformations","Adds non-linearity.",0.3,DeepLearning,Transformer,FFN
"What is model parallelism?",multiple_choice,"Split model across devices,Split data only,Single GPU training,Compression","Split model across devices","Used for very large models.",0.4,MLOps,DistributedTraining,ModelParallelism
"What is data parallelism?",multiple_choice,"Split data across devices,Split layers,Reduce parameters,Freeze weights","Split data across devices","Most common distributed strategy.",0.3,MLOps,DistributedTraining,DataParallelism
"What bottleneck affects data parallel training?",multiple_choice,"Gradient synchronization,Model size only,Disk speed,Activation functions","Gradient synchronization","Communication overhead limits scaling.",0.4,MLOps,DistributedTraining,Communication
"What is mixed precision training?",multiple_choice,"Using FP16 and FP32,Only FP64,Quantization only,Dropout technique","Using FP16 and FP32","Speeds training and saves memory.",0.4,DeepLearning,Optimization,MixedPrecision
"What is a risk of mixed precision?",multiple_choice,"Numerical instability,Lower accuracy always,Overfitting,Slower training","Numerical instability","Requires loss scaling.",0.4,DeepLearning,Optimization,MixedPrecision
"What is gradient clipping used for?",multiple_choice,"Prevent exploding gradients,Speed inference,Normalize data,Reduce parameters","Prevent exploding gradients","Limits gradient magnitude.",0.3,DeepLearning,Optimization,GradientClipping
"What is the main purpose of Adam optimizer?",multiple_choice,"Adaptive learning rates,Second-order optimization,Exact convergence,Batch normalization","Adaptive learning rates","Combines momentum and RMSProp.",0.3,DeepLearning,Optimization,Adam
"What is a drawback of Adam?",multiple_choice,"May generalize worse than SGD,Too slow,Hard to implement,No convergence","May generalize worse than SGD","Sometimes inferior final performance.",0.4,DeepLearning,Optimization,Adam
"What is SGD with momentum?",multiple_choice,"SGD with velocity term,Adaptive optimizer,Second-order method,Regularization","SGD with velocity term","Accelerates convergence.",0.3,DeepLearning,Optimization,SGD
"What does learning rate scheduling do?",multiple_choice,"Adjust learning rate over time,Fix learning rate,Normalize gradients,Reduce batch size","Adjust learning rate over time","Improves convergence.",0.3,DeepLearning,Optimization,LearningRateSchedule
"What happens if learning rate is too high?",multiple_choice,"Training diverges,Training slows,Underfitting,No effect","Training diverges","Updates overshoot minima.",0.3,DeepLearning,Optimization,LearningRate
"What happens if learning rate is too low?",multiple_choice,"Very slow convergence,Overfitting,Exploding gradients,No learning","Very slow convergence","Training takes long time.",0.3,DeepLearning,Optimization,LearningRate
"What is the exploding gradient problem?",multiple_choice,"Gradients grow uncontrollably,Gradients vanish,Loss becomes zero,Model underfits","Gradients grow uncontrollably","Large gradients destabilize training.",0.4,DeepLearning,Optimization,ExplodingGradient
"Which technique directly addresses exploding gradients?",multiple_choice,"Gradient clipping,Dropout,Batch normalization,Early stopping","Gradient clipping","It caps gradient magnitude.",0.3,DeepLearning,Optimization,GradientClipping
"Why are residual connections effective?",multiple_choice,"Improve gradient flow,Reduce parameters,Add regularization,Normalize data","Improve gradient flow","They ease training of deep networks.",0.4,DeepLearning,Architecture,Residual
"What problem do residual networks mitigate?",multiple_choice,"Degradation problem,Overfitting,Class imbalance,Data leakage","Degradation problem","Deeper models perform worse without residuals.",0.4,DeepLearning,Architecture,ResNet
"What is the role of skip connections?",multiple_choice,"Bypass layers for signal flow,Increase depth only,Reduce batch size,Compress features","Bypass layers for signal flow","They preserve information.",0.3,DeepLearning,Architecture,SkipConnection
"What is an autoencoder mainly used for?",multiple_choice,"Dimensionality reduction,Classification only,Regression,Reinforcement learning","Dimensionality reduction","Learns compressed representations.",0.3,DeepLearning,Autoencoder,Usage
"What is a bottleneck layer?",multiple_choice,"Lowest dimensional latent layer,Output layer,Input layer,Normalization layer","Lowest dimensional latent layer","Forces compression.",0.3,DeepLearning,Autoencoder,Bottleneck
"What is a denoising autoencoder?",multiple_choice,"Reconstruct clean input from noisy data,Remove labels,Increase noise,Classify images","Reconstruct clean input from noisy data","Improves robustness.",0.4,DeepLearning,Autoencoder,Denoising
"What is a variational autoencoder?",multiple_choice,"Probabilistic generative model,Deterministic compressor,Classifier,Optimizer","Probabilistic generative model","Learns latent distributions.",0.6,DeepLearning,GenerativeModel,VAE
"What loss is used in VAE?",multiple_choice,"Reconstruction plus KL divergence,MSE only,Cross-entropy only,Hinge loss","Reconstruction plus KL divergence","Balances fidelity and regularization.",0.6,DeepLearning,GenerativeModel,VAE
"What is GAN training based on?",multiple_choice,"Minimax game,Maximum likelihood,Reinforcement learning only,Clustering","Minimax game","Generator and discriminator compete.",0.5,DeepLearning,GenerativeModel,GAN
"What is mode collapse in GANs?",multiple_choice,"Generator produces limited outputs,Training diverges,Discriminator fails,Loss is zero","Generator produces limited outputs","Diversity is lost.",0.5,DeepLearning,GenerativeModel,GAN
"What stabilizes GAN training?",multiple_choice,"Wasserstein loss,Sigmoid activation,High learning rate,Large batch size","Wasserstein loss","Improves gradient behavior.",0.6,DeepLearning,GenerativeModel,WGAN
"What is reinforcement learning mainly concerned with?",multiple_choice,"Sequential decision making,Supervised prediction,Clustering,Dimensionality reduction","Sequential decision making","Agents interact with environments.",0.3,ReinforcementLearning,Concept,Basics
"What defines a Markov Decision Process?",multiple_choice,"States, actions, rewards, transitions,Neural networks only,Loss functions,Gradients","States, actions, rewards, transitions","Formal RL framework.",0.4,ReinforcementLearning,MDP,Definition
"What is a policy in RL?",multiple_choice,"Mapping from states to actions,Reward function,Environment model,Loss","Mapping from states to actions","Defines agent behavior.",0.3,ReinforcementLearning,Policy,Definition
"What is the goal of RL?",multiple_choice,"Maximize cumulative reward,Minimize loss,Classify states,Predict transitions","Maximize cumulative reward","Long-term objective.",0.3,ReinforcementLearning,Objective,Return
"What is exploration vs exploitation trade-off?",multiple_choice,"Trying new actions vs using known best,Training vs testing,Speed vs accuracy,Bias vs variance","Trying new actions vs using known best","Balances learning and reward.",0.4,ReinforcementLearning,Strategy,Exploration
"What does epsilon-greedy do?",multiple_choice,"Random actions with probability epsilon,Always exploit,Always explore,Decay rewards","Random actions with probability epsilon","Simple exploration strategy.",0.3,ReinforcementLearning,Policy,EpsilonGreedy
"What is Q-learning?",multiple_choice,"Value-based off-policy method,Policy gradient,Model-based planning,Supervised learning","Value-based off-policy method","Learns action-value function.",0.4,ReinforcementLearning,ValueBased,QLearning
"What does the Q-function represent?",multiple_choice,"Expected return of state-action pair,Immediate reward only,Policy probability,State transition","Expected return of state-action pair","Estimates long-term value.",0.3,ReinforcementLearning,ValueFunction,QFunction
"What is SARSA?",multiple_choice,"On-policy TD control algorithm,Off-policy method,Model-based RL,Policy gradient","On-policy TD control algorithm","Uses current policy actions.",0.4,ReinforcementLearning,ValueBased,SARSA
"What is the difference between on-policy and off-policy?",multiple_choice,"Whether target policy equals behavior policy,Learning rate,Model type,Reward scale","Whether target policy equals behavior policy","Key RL distinction.",0.4,ReinforcementLearning,Theory,PolicyType
"What is policy gradient method?",multiple_choice,"Optimize policy directly,Estimate value only,Use Q-table,Clustering","Optimize policy directly","Uses gradient ascent.",0.4,ReinforcementLearning,PolicyGradient,Concept
"What is REINFORCE algorithm?",multiple_choice,"Monte Carlo policy gradient,Value iteration,Actor-critic only,DQN variant","Monte Carlo policy gradient","Uses episodic returns.",0.5,ReinforcementLearning,PolicyGradient,REINFORCE
"What is actor-critic?",multiple_choice,"Policy and value combined,Only value learning,Only policy learning,Supervised model","Policy and value combined","Reduces variance.",0.4,ReinforcementLearning,ActorCritic,Architecture
"What is advantage function?",multiple_choice,"Q minus V value,Reward only,Policy probability,Loss","Q minus V value","Measures relative action quality.",0.5,ReinforcementLearning,ActorCritic,Advantage
"What problem does DQN solve?",multiple_choice,"Large state spaces,Continuous actions,Partial observability,Multi-agent RL","Large state spaces","Uses neural networks for Q.",0.4,ReinforcementLearning,DeepRL,DQN
"Why use experience replay?",multiple_choice,"Break correlation of samples,Increase exploration,Reduce memory,Speed inference","Break correlation of samples","Stabilizes learning.",0.4,ReinforcementLearning,DeepRL,Replay
"What is target network used for?",multiple_choice,"Stabilize Q-learning,Increase speed,Reduce memory,Improve exploration","Stabilize Q-learning","Fixes target temporarily.",0.4,ReinforcementLearning,DeepRL,TargetNetwork
"What is overestimation bias in DQN?",multiple_choice,"Q-values become too large,Rewards vanish,No convergence,Underfitting","Q-values become too large","Max operator bias.",0.5,ReinforcementLearning,DeepRL,Overestimation
"How does Double DQN help?",multiple_choice,"Decouple action selection and evaluation,Increase exploration,Reduce memory,Speed training","Decouple action selection and evaluation","Reduces overestimation.",0.5,ReinforcementLearning,DeepRL,DoubleDQN
"What is PPO?",multiple_choice,"Stable policy optimization method,Value-based algorithm,Model-based RL,Clustering","Stable policy optimization method","Constrains policy updates.",0.5,ReinforcementLearning,PolicyGradient,PPO
"What is the benefit of PPO?",multiple_choice,"Stable and simple training,Exact optimality,Faster inference,No hyperparameters","Stable and simple training","Widely used in practice.",0.4,ReinforcementLearning,PolicyGradient,PPO
"What is model-based RL?",multiple_choice,"Learn environment dynamics,No rewards needed,Supervised learning,Only simulation","Learn environment dynamics","Enables planning.",0.5,ReinforcementLearning,ModelBased,Concept
"What is the main challenge of RL?",multiple_choice,"Sample inefficiency,Label noise,Feature scaling,Overfitting","Sample inefficiency","Requires many interactions.",0.4,ReinforcementLearning,Challenge,SampleEfficiency
"What is reward shaping?",multiple_choice,"Modify rewards to guide learning,Remove rewards,Normalize states,Clip gradients","Modify rewards to guide learning","Speeds convergence.",0.4,ReinforcementLearning,Technique,RewardShaping`;
